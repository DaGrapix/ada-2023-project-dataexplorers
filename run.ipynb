{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.580519200Z",
     "start_time": "2023-11-17T13:19:53.233712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aymer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aymer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import pickle as pkl\n",
    "import time\n",
    "from abc import ABC\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7367776b7892e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f088f2d89134c22b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.580519200Z",
     "start_time": "2023-11-17T13:19:53.810185900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/MovieSummaries/'\n",
    "ADDITIONAL_FOLDER = 'data/AdditionalData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27f79ecc817d6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.581765100Z",
     "start_time": "2023-11-17T13:19:53.833880100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# reading a txt file and convert it to a dataframe\n",
    "def read_txt(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a53e38e4a5d0f068",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.734406200Z",
     "start_time": "2023-11-17T13:19:53.859833700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# importing the data\n",
    "plots = pd.read_csv(DATA_FOLDER + 'plot_summaries.txt',header=None, sep=\"\\t\")\n",
    "movies = pd.read_csv(DATA_FOLDER + 'movie.metadata.tsv',header=None, sep=\"\\t\")\n",
    "characters = pd.read_csv(DATA_FOLDER + 'character.metadata.tsv',header=None, sep=\"\\t\")\n",
    "names = pd.read_csv(DATA_FOLDER + 'name.clusters.txt',header=None, sep=\"\\t\")\n",
    "tvtropes = pd.read_csv(DATA_FOLDER + 'tvtropes.clusters.txt',header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9115b319f772298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.734406200Z",
     "start_time": "2023-11-17T13:19:56.287806400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# renaming columns the plots dataframe\n",
    "plots.columns = ['wikipedia_movie_id', 'plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69660aa6a6d729",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "`movies` data\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dba498a527e8e1b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.734406200Z",
     "start_time": "2023-11-17T13:19:56.302744900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# renaming the movie dataframe columns\n",
    "movies.columns = ['wikipedia_movie_id', 'freebase_movie_id', 'name', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f338f80897a3b1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "`characters` data:\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f75251129d65070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.735407500Z",
     "start_time": "2023-11-17T13:19:56.321392500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# fixing the character columns\n",
    "characters.columns = ['wikipedia_movie_id', 'freebase_movie_id', 'release_date', 'character_name', 'date_of_birth', 'gender', 'height', 'ethnicity', 'name', 'age_at_release', 'freebase_character_map_id', 'freebase_character_id', 'freebase_actor_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef974f701d6c7f62",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a4ed33a7f128d544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.735407500Z",
     "start_time": "2023-11-17T13:19:56.333456400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies without revenue data:  73340\n"
     ]
    }
   ],
   "source": [
    "revenue_nans = movies[movies['revenue'].isna()][\"wikipedia_movie_id\"]\n",
    "print('Number of movies without revenue data: ', revenue_nans.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cec91c4726c2d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scraping\n",
    "\n",
    "A lot of the movies have empty Box Office Revenue so one idea ot fix this is to acquire that data through other means.\n",
    "\n",
    "Here we scraped the ImDB website to retrieve some of these informations about movie revenues as well as additional interesting informations such as movie ratings, movie producers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6840eb5b426ed82b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.735407500Z",
     "start_time": "2023-11-17T13:19:56.366619900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import imdb_scraper as imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c37714eeaa9827",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Example on how to use the scraper\n",
    "\n",
    "<b><span style=\"color:red\">Don't close the browser when it is in use </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96ca96099925efb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.743790800Z",
     "start_time": "2023-11-17T13:19:56.636051700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global_revenue': None,\n",
       " 'budget': 2846000.0,\n",
       " 'gross_domestic': None,\n",
       " 'opening_weekend': None,\n",
       " 'rating_score': 6.7,\n",
       " 'number_of_ratings': 1700.0,\n",
       " 'watched_rank': None,\n",
       " 'producer': 'Robert Wise',\n",
       " 'release_year': None}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the scraper object\n",
    "myscraper = imdb.ImdbScraper()\n",
    "\n",
    "# select a movie wikipedia id\n",
    "movie_id = movies.iloc[74623][\"wikipedia_movie_id\"]\n",
    "\n",
    "# scrape the movie infos\n",
    "scraped_data = myscraper.get_imdb_infos(movie_id)\n",
    "\n",
    "# close the browser\n",
    "myscraper.close()\n",
    "\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1716cbcf7b0d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Let the scraping begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1bf069c57a541",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Computing `n_computer` partitions to parallelize ImDB's scraping on multiple computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "70ea312e0cf0af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.743790800Z",
     "start_time": "2023-11-17T13:20:03.446492700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_computer  = 6\n",
    "size        = n//n_computer\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_computer computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_computer)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_computer*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb84e01413d60fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.743790800Z",
     "start_time": "2023-11-17T13:20:03.469078Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 13623\n",
      "\n",
      "partition 0: (0, 13622)\n",
      "partition 1: (13623, 27245)\n",
      "partition 2: (27246, 40868)\n",
      "partition 3: (40869, 54491)\n",
      "partition 4: (54492, 68114)\n",
      "partition 5: (68115, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_computer)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e1aada2c19af6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Each user should use its index to compute his attributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4cd09e2661fb8458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.743790800Z",
     "start_time": "2023-11-17T13:20:03.493075200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select your index here:\n",
    "    - Anthony   0 & 1\n",
    "    - Anton     2\n",
    "    - Aymeric   3\n",
    "    - Eric      4\n",
    "    - Yara      5\n",
    "\"\"\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25496951ced96f1b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Since ImDB has a protection against bots, using `requests` yields a `forbidden` error. The use of `selenium` to simmulate a real human operator avoids this problem and allows to scrape, though more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e82b9b26c7d5e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.743790800Z",
     "start_time": "2023-11-17T13:20:03.519074400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def scrape_partition(index):\n",
    "    \"\"\"\n",
    "    Scrape the IMDB data for the movies in the partition with the given index.\n",
    "    The scraped dataset is saved in a separate csv file for each partition.\n",
    "\n",
    "    Input:\n",
    "    index(int) : The index of the partition to scrape.\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # create a header dataframe\n",
    "    header = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"name\", \"global_revenue\", \"budget\", \"gross_domestic\", \"opening_weekend\", \n",
    "                               \"rating_score\", \"number_of_ratings\", \"watched_rank\", \"producer\", \"release_year\"])  \n",
    "    \n",
    "    # csv file name\n",
    "    csv_file = ADDITIONAL_FOLDER + 'imdb_partitions' + '/imdb_scraped_data_' + str(index) + '.csv'\n",
    "    \n",
    "    #initialize the scraper object\n",
    "    myscraper = imdb.ImdbScraper()\n",
    "    \n",
    "    # if the csv_file doesn't exist, create it and write the header\n",
    "    if not os.path.isfile(csv_file):\n",
    "        header.to_csv(csv_file, index=False)\n",
    "        starting_index = 0\n",
    "    else:\n",
    "        scraped = pd.read_csv(csv_file)\n",
    "        starting_index = scraped.shape[0]\n",
    "       \n",
    "    index_range = partitions[index][starting_index:]\n",
    "    \n",
    "    for i in index_range:\n",
    "        movie_id = movies.iloc[i][\"wikipedia_movie_id\"]\n",
    "    \n",
    "        movie_infos = myscraper.get_imdb_infos(movie_id)\n",
    "        row = [movie_id, movies[\"name\"].values[i], *list(movie_infos.values())]\n",
    "    \n",
    "        # create a new DataFrame for the row\n",
    "        row_df = pd.DataFrame([row], columns=header.columns)\n",
    "    \n",
    "        # replace None values with the string 'None'\n",
    "        row_df = row_df.fillna('None')\n",
    "    \n",
    "        # append the row DataFrame to the CSV file\n",
    "        row_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    myscraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "230278d2e2005a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.744802Z",
     "start_time": "2023-11-17T13:20:03.539083400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# run the scraper\n",
    "scrape_partition(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7266ab3a4f157e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Multi-thread version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49c97a5eb84d11",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This part shows an implementation of the scraper running in parallel on multiple threads on a single computer. The number of threads `n_threads` can be increased if the user's computer has access to more threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73fa685679dade63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.744802Z",
     "start_time": "2023-11-17T13:20:04.893416600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "40682fe69ecf99d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.792621Z",
     "start_time": "2023-11-17T13:20:04.913701700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_threads   = 6\n",
    "size        = n//n_threads\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_threads computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_threads)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_threads*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5bf50ee152194ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:07.793130300Z",
     "start_time": "2023-11-17T13:20:04.946208400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 13623\n",
      "\n",
      "partition 0: (0, 13622)\n",
      "partition 1: (13623, 27245)\n",
      "partition 2: (27246, 40868)\n",
      "partition 3: (40869, 54491)\n",
      "partition 4: (54492, 68114)\n",
      "partition 5: (68115, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_threads)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38604fa01210b1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:09.638742500Z",
     "start_time": "2023-11-17T13:20:04.963233600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "threads = []\n",
    "\n",
    "for i in range(n_threads):\n",
    "    threads.append(threading.Thread(target=scrape_partition, args=(i,)))\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11736bfa81b06b9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Merging the scraped partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "93773db95c5e1f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:12.450667900Z",
     "start_time": "2023-11-17T13:20:09.187334200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "csv_file = ADDITIONAL_FOLDER + r\"/imdb_scraped_dataset.csv\"\n",
    "n_computer  = 6\n",
    "try:\n",
    "    partition_dataset = pd.read_csv(csv_file)\n",
    "except:\n",
    "    partition_file = ADDITIONAL_FOLDER + r'imdb_partitions/imdb_scraped_data_'\n",
    "    partition_dataset = pd.read_csv(partition_file + '0.csv')\n",
    "\n",
    "    for i in range(1, n_computer):\n",
    "        partition = partition_file + str(i) + '.csv'\n",
    "        partition_dataset = pd.concat([partition_dataset, pd.read_csv(partition)], axis=0).fillna('None')\n",
    "        partition_dataset.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b4d0685659628",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Completing missing revenue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "35acb887f5fbc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:12.747650300Z",
     "start_time": "2023-11-17T13:20:12.442913200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipedia_movie_id</th>\n",
       "      <th>name</th>\n",
       "      <th>global_revenue</th>\n",
       "      <th>budget</th>\n",
       "      <th>gross_domestic</th>\n",
       "      <th>opening_weekend</th>\n",
       "      <th>rating_score</th>\n",
       "      <th>number_of_ratings</th>\n",
       "      <th>watched_rank</th>\n",
       "      <th>producer</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>975900</td>\n",
       "      <td>Ghosts of Mars</td>\n",
       "      <td>14010832.0</td>\n",
       "      <td>28000000.0</td>\n",
       "      <td>8709640.0</td>\n",
       "      <td>3804452.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>John Carpenter</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3196793</td>\n",
       "      <td>Getting Away with Murder: The JonBenét Ramsey ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Edward Lucas</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28463795</td>\n",
       "      <td>Brun bitter</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Sølve Skagen</td>\n",
       "      <td>1988.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wikipedia_movie_id                                               name  \\\n",
       "0              975900                                     Ghosts of Mars   \n",
       "1             3196793  Getting Away with Murder: The JonBenét Ramsey ...   \n",
       "2            28463795                                        Brun bitter   \n",
       "\n",
       "  global_revenue      budget gross_domestic opening_weekend rating_score  \\\n",
       "0     14010832.0  28000000.0      8709640.0       3804452.0          4.9   \n",
       "1           None        None           None            None          6.0   \n",
       "2           None        None           None            None          5.6   \n",
       "\n",
       "  number_of_ratings watched_rank        producer release_year  \n",
       "0           57000.0       3850.0  John Carpenter       2001.0  \n",
       "1              69.0         None    Edward Lucas       2000.0  \n",
       "2              40.0         None    Sølve Skagen       1988.0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the scraped dataset\n",
    "imdb_movies = pd.read_csv(ADDITIONAL_FOLDER + 'imdb_scraped_dataset.csv')\n",
    "imdb_movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1c933bea133efd4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:12.848102300Z",
     "start_time": "2023-11-17T13:20:12.747650300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies without global revenue data:  0\n"
     ]
    }
   ],
   "source": [
    "# computing number of movies without global revenue data in the imdb dataset\n",
    "global_revenue_nans = imdb_movies[imdb_movies['global_revenue'].isna()][\"wikipedia_movie_id\"]\n",
    "print('Number of movies without global revenue data: ', global_revenue_nans.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3d0fa3eac95cb",
   "metadata": {},
   "source": [
    "We have less missing revenue data in the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f04c655e2ce5118e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:12.985717Z",
     "start_time": "2023-11-17T13:20:12.781390200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: wikipedia_movie_id, dtype: int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating number of movies that have missing revenue in the original dataset but don't have missing revenue in the imdb dataset\n",
    "(~global_revenue_nans.isin(revenue_nans)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb550ddfb576ba07",
   "metadata": {},
   "source": [
    "1444 scraped movies have revenue informations not present in out original dataset. We can thus complete our revenue data with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605031eb1a19348",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Completting missing date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b656ac1b746a71a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:13.241907800Z",
     "start_time": "2023-11-17T13:20:12.810867600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains: .0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#imdb_movies.drop(columns=['name'], inplace=True)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#imdb_movies[\"release_year\"] = imdb_movies[\"release_year\"].apply(lambda x: str(x)[:4] if x is not None else x)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#imdb_movies[\"imdb_release_date\"] = pd.to_datetime(imdb_movies[\"release_year\"], errors='coerce', format=\"%Y\")\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m imdb_movies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_release_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_movies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelease_year\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m movies_augmented \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(movies, imdb_movies, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia_movie_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m movies_augmented\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1064\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1062\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(tz)\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1064\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1066\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:229\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    227\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 229\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:430\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_to_datetime_with_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_datetime_format\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:538\u001b[0m, in \u001b[0;36m_to_datetime_with_format\u001b[1;34m(arg, orig_arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _box_as_indexlike(result, utc\u001b[38;5;241m=\u001b[39mutc, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fallback\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_datetime_format\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:473\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, tz, fmt, exact, errors, infer_datetime_format)\u001b[0m\n\u001b[0;32m    470\u001b[0m utc \u001b[38;5;241m=\u001b[39m tz \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutOfBoundsDatetime:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\strptime.pyx:156\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unconverted data remains: .0"
     ]
    }
   ],
   "source": [
    "#imdb_movies.drop(columns=['name'], inplace=True)\n",
    "#imdb_movies[\"release_year\"] = imdb_movies[\"release_year\"].apply(lambda x: str(x)[:4] if x is not None else x)\n",
    "\n",
    "#imdb_movies[\"imdb_release_date\"] = pd.to_datetime(imdb_movies[\"release_year\"], errors='coerce', format=\"%Y\")\n",
    "imdb_movies[\"imdb_release_date\"] = pd.to_datetime(imdb_movies[\"release_year\"], format=\"%Y\")\n",
    "\n",
    "movies_augmented = pd.merge(movies, imdb_movies, on=\"wikipedia_movie_id\")\n",
    "movies_augmented.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564189e65056ee5a",
   "metadata": {},
   "source": [
    "Similarly to the previous part, we see that the scraping was able to retrieve 1711 release years that were missing in our original data. We can similarly complete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa105df09265f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:13.265415600Z",
     "start_time": "2023-11-17T13:20:12.904064200Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_dates = movies_augmented[movies_augmented['release_date'].isna()].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4359b45d89dca7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:13.320417100Z",
     "start_time": "2023-11-17T13:20:12.937186500Z"
    }
   },
   "outputs": [],
   "source": [
    "# filling the missing dates with the imdb dates\n",
    "movies_augmented.loc[missing_dates, \"release_date\"] = movies_augmented.loc[missing_dates, \"imdb_release_date\"]\n",
    "\n",
    "# replace NaT with nans\n",
    "movies_augmented[\"release_date\"].where(movies_augmented[\"release_date\"].notnull(), None, inplace=True)\n",
    "movies_augmented.drop(columns=['release_year', 'imdb_release_date'], inplace=True)\n",
    "movies_augmented.loc[missing_dates, \"release_date\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a6d1456565f8b",
   "metadata": {},
   "source": [
    "We can also create a year feature in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa11668314a51d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:13.320417100Z",
     "start_time": "2023-11-17T13:20:12.997722Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_augmented.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53842c339a5b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:26.693428200Z",
     "start_time": "2023-11-17T13:20:13.033873600Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"extracting the year from the release date feature\"\"\"\n",
    "\n",
    "# extracting the years from the release date feature\n",
    "movie_with_date = movies_augmented[-movies_augmented[\"release_date\"].isna()].copy(deep=True)\n",
    "dates = movie_with_date[\"release_date\"]\n",
    "date_years = dates.astype(str).str.extract(r'(\\d{4})')\n",
    "\n",
    "# adding an index column & renaiming the date_years columns\n",
    "date_years[\"index\"] = date_years.index\n",
    "date_years.columns = [\"year\", \"index\"]\n",
    "\n",
    "# adding an index column to the movies_augmented dataframe\n",
    "movies_augmented[\"index\"] = movies_augmented.index\n",
    "\n",
    "# merging the movies_augmented and date_years dataframes on the index column\n",
    "merged = pd.merge(movies_augmented, date_years, on=\"index\", how=\"left\")\n",
    "merged.drop(columns=[\"index\"], inplace=True)\n",
    "movies_augmented = merged.copy(deep=True)\n",
    "movies_augmented.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12290feb3bffa838",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:26.937626200Z",
     "start_time": "2023-11-17T13:20:26.676430300Z"
    }
   },
   "outputs": [],
   "source": [
    "# filling the missing dates with the imdb dates\n",
    "missing_revenue = movies_augmented[movies_augmented['revenue'].isna()].index.values\n",
    "movies_augmented.loc[missing_revenue, \"revenue\"] = movies_augmented.loc[missing_revenue, \"global_revenue\"]\n",
    "movies_augmented.loc[missing_revenue, \"revenue\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b9451226e3354",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Removing useless data\n",
    "We shouldn't consider movies with no revenue data, no valid date or no valid ratingwe shouldn't consider movies with no revenue data, no valid date or no valid rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d51d322f21a651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:26.938629900Z",
     "start_time": "2023-11-17T13:20:26.753001800Z"
    }
   },
   "outputs": [],
   "source": [
    "#droping missing revenue, date & rating datapoints\n",
    "revenue_nans_filter         = movies_augmented['revenue'].isna()\n",
    "release_date_nans_filter    = movies_augmented['release_date'].isna()\n",
    "rating_nans_filter          = movies_augmented['rating_score'].isna()\n",
    "\n",
    "movies_augmented = movies_augmented[~(revenue_nans_filter | release_date_nans_filter | rating_nans_filter)].copy(deep=True)\n",
    "\n",
    "# dropping the global_revenue column\n",
    "movies_augmented.drop(columns=[\"global_revenue\"], inplace=True)\n",
    "\n",
    "# checking we removed all the useless datapoints\n",
    "missing_revenue = movies_augmented[\"revenue\"].isna().sum()\n",
    "missing_dates = movies_augmented[\"release_date\"].isna().sum()\n",
    "missing_rating = movies_augmented[\"rating_score\"].isna().sum()\n",
    "\n",
    "print(f\"Missing release date datapoints: {missing_revenue}\")\n",
    "print(f\"Missing rating datapoints: {missing_dates}\")\n",
    "print(f\"Missing revenue datapoints: {missing_rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f363c81a73899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.011584Z",
     "start_time": "2023-11-17T13:20:26.796383Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_augmented.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5196d844a16e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.011584Z",
     "start_time": "2023-11-17T13:20:26.829060200Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88570c4d9ea0af",
   "metadata": {},
   "source": [
    "We are left with close to 18000 clean datapoints, which is a lot less than the total numbers of movies we had at the beginning. But this is necessary to correctly define the next analysis steps we will conduct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb0b039677bf33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.083328500Z",
     "start_time": "2023-11-17T13:20:26.845058300Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting years to floats & defining a new dataframe\n",
    "movies = movies_augmented.copy(deep=True)\n",
    "movies[\"year\"] = movies[\"year\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d869d2a371e4dc2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Taking care of inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa57b793ecd8d0",
   "metadata": {},
   "source": [
    "Comparing movie revenues across different periods of time is not relevant as the value of the USD has varied inbetween. This calls for taking inflation into account and adjusting revenues across all time periods on a single reference date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b84b475332b7db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.281451700Z",
     "start_time": "2023-11-17T13:20:26.879619300Z"
    }
   },
   "outputs": [],
   "source": [
    "# inflation data from:\n",
    "# https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/\n",
    "us_inflation = pd.read_csv(ADDITIONAL_FOLDER + \"US_yearly_inflation.csv\")\n",
    "us_yearly_inflation = us_inflation[[\"Year\", \"Avg-Avg\"]].fillna(0)\n",
    "us_yearly_inflation.rename(columns={\"Year\": \"year\", \"Avg-Avg\": \"avg_avg\"}, inplace=True)\n",
    "us_yearly_inflation.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6b42b4fb2b4df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.282449200Z",
     "start_time": "2023-11-17T13:20:26.906804300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the 1897 datapoint needs to be removed as we have no inflation data for before 1913\n",
    "movies = movies[movies[\"year\"] >= 1913]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a43e4c7592842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.715364300Z",
     "start_time": "2023-11-17T13:20:26.922794300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plotting the yearly USD Inflation Rates\n",
    "plt.plot(us_yearly_inflation['year'], us_yearly_inflation['avg_avg'])\n",
    "plt.title(\"Yearly USD Inflation Rates (From Average to Average)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Inflation Rate (%)\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb69a25a9890b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.715364300Z",
     "start_time": "2023-11-17T13:20:27.238919500Z"
    }
   },
   "outputs": [],
   "source": [
    "prices = [1] # we set the initial price as a reference\n",
    "\n",
    "# Calculate prices based on inflation rates\n",
    "for index, row in us_yearly_inflation.iterrows():\n",
    "    if index == 0:\n",
    "        continue # Skip the first row (no inflation data for the initial price)\n",
    "    price = prices[-1] * (1 + row['avg_avg'] / 100)\n",
    "    prices.append(price)\n",
    "\n",
    "# Add the prices to the DataFrame\n",
    "us_yearly_inflation['price'] = prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae22e8a5b9e1e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:27.806110500Z",
     "start_time": "2023-11-17T13:20:27.254921900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reference_price = us_yearly_inflation[us_yearly_inflation[\"year\"]==2012][\"price\"].values[0]\n",
    "correction_factor = reference_price / us_yearly_inflation[\"price\"]\n",
    "us_yearly_inflation[\"correlation_factor\"] = correction_factor\n",
    "us_yearly_inflation.drop(columns=['price', 'avg_avg'], inplace=True)\n",
    "\n",
    "merged_all = pd.merge(movies, us_yearly_inflation, on=\"year\", how=\"left\")\n",
    "movies = merged_all.copy(deep=True)\n",
    "\n",
    "# inflation adjusted revenue\n",
    "movies[\"adjusted_revenue\"] = movies[\"revenue\"] * movies[\"correlation_factor\"]\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adb83b5853061",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Box office revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce348983111cff",
   "metadata": {},
   "source": [
    "We need to adjust movie revenues to inflation to make them comparable. As we have inflation data up to 2012, we will place our reference USD value at year 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc076a1b7d0f18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.244561Z",
     "start_time": "2023-11-17T13:20:27.317452600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the max box office revenue for each year\n",
    "max_revenue_per_year = movies.groupby('year')['revenue'].max().reset_index()\n",
    "max_revenue_per_year_adjusted = movies.groupby('year')['adjusted_revenue'].max().reset_index()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "# Create a line plot\n",
    "axes[0].plot(max_revenue_per_year['year'], max_revenue_per_year['revenue'])\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Max Revenue')\n",
    "axes[0].set_title('Max Box Office Revenue per Year for non-adjusted inflation')\n",
    "plt.grid(False)\n",
    "\n",
    "# Create a line plot\n",
    "axes[1].plot(max_revenue_per_year_adjusted['year'], max_revenue_per_year_adjusted['adjusted_revenue'])\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Max Revenue')\n",
    "axes[1].set_title('Max Box Office Revenue per Year for adjusted inflation')\n",
    "plt.grid(False)\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ab326e1d93861",
   "metadata": {},
   "source": [
    "We can plot the max box office revenue for every year. Note that we are interested in the best movies so we don't plot average or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3809b062477215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.245834500Z",
     "start_time": "2023-11-17T13:20:27.939595Z"
    }
   },
   "outputs": [],
   "source": [
    "# movie with most revenue\n",
    "highest_revenue_movie = movies[movies[\"revenue\"]==movies[\"revenue\"].max()][\"name\"].values[0]\n",
    "\n",
    "# movie with most revenue\n",
    "highest_adjusted_revenue_movie = movies[movies[\"adjusted_revenue\"]==movies[\"adjusted_revenue\"].max()][\"name\"].values[0]\n",
    "\n",
    "print(f\"Highest revenue movie: {highest_revenue_movie}\")\n",
    "print(f\"Highest adjusted revenue movie: {highest_adjusted_revenue_movie}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef12336555fc23",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Looking at the previous graph it seems like Avatar is the best movie! Until we adjust the revenue for inflation and see that it is actually Snow White and the Seven Dwarfs that is the movie that generated the most value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def32a6317a5237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.246842Z",
     "start_time": "2023-11-17T13:20:27.955005300Z"
    }
   },
   "outputs": [],
   "source": [
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac3f94ba2e2352",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Looking for a correlation between the rating and the revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19706d73a6fc9bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.246842Z",
     "start_time": "2023-11-17T13:20:27.988127900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pearson_corr = stats.pearsonr(movies['rating_score'], movies['adjusted_revenue'])\n",
    "\n",
    "print('Pearson correlation coefficient: ', pearson_corr)\n",
    "print('As we can see there is a weak correlation between the rating and the revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a92925e0f3a3e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The null-hypothesis states that there is no correlation between movie revenue and ratings. Based on this Pearson Correlation test, the p-value is smaller than 0.05, so we reject the null-hypothesis. However, the correlation coefficient is equal to 0.14, which is quite small. So we conclude that there is not a significant correlation between movie revenue and ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479107f7536744e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Therefore, we can define a success movie score, which combines both the revenue and ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f49f6dcbc0ff2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We define the score as a function composed of two components:\n",
    "\n",
    "- $BORC$    : Box Office Revenue Component\n",
    "- $RC$      : Rating Component\n",
    "\n",
    "These components can be computed with the Adjusted Movie Revenue (which we write define as $AMR$) and the movie rating (we will write as $MR$)\n",
    "\n",
    "We log-transform the data and define $x = \\log(AMR)$ and $y = MR$\n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "    BORC = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\\\\\n",
    "    RC = \\frac{y - \\min(y)}{\\max(y) - \\min(y)}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "As such, we have $BORC, RC \\in [0, 1]$\n",
    "\n",
    "We define a weight $\\alpha \\in [0, 1]$ and take the convex combinations of $BORC$ and $RC$. This weight controls the importance we give to each of our two components in our metric. Its importance will be determined in the future and we will adjust it accordingly to the study. Multiplying by 100 gives us a final score \n",
    "\n",
    "$$Movie Score = 100\\left(\\alpha BORC + (1 - \\alpha) RC \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e37fa180a6243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.355568200Z",
     "start_time": "2023-11-17T13:20:28.005127400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the components\n",
    "log_adj_revenue = np.log(movies['adjusted_revenue'].values)\n",
    "movie_rating = movies['rating_score'].values\n",
    "\n",
    "movies.loc[:, 'BORC'] = (log_adj_revenue - np.min(log_adj_revenue)) / (np.max(log_adj_revenue) - np.min(log_adj_revenue))\n",
    "movies.loc[:, 'RC'] = (movie_rating - np.min(movie_rating)) / (np.max(movie_rating) - np.min(movie_rating))\n",
    "\n",
    "# Calculate Movie Score\n",
    "alpha = 1/2\n",
    "movies.loc[:, 'movie_score'] = 100 * (alpha*movies['BORC'] + (1 - alpha)*movies['RC'])\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325cb6fb18c375",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Now that we have the movie scores we can keep the top T percent and consider these as the Good films that we want to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5d9ae8d18c288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.382114600Z",
     "start_time": "2023-11-17T13:20:28.032135700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "T = 75\n",
    "num_movies  = movies.shape[0]\n",
    "movies      = movies.sort_values(\"movie_score\", ascending = False)\n",
    "\n",
    "df_good_movies  = movies[movies[\"movie_score\"] >= T].copy()\n",
    "df_bad_movies   = movies[movies[\"movie_score\"] < T].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9112d243bfdc5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.383110100Z",
     "start_time": "2023-11-17T13:20:28.086664600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_good_movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54874f98301010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.384111300Z",
     "start_time": "2023-11-17T13:20:28.096664600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_bad_movies.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d4f9dbfe678bed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We now have 2 datasets: of good movies and of bad ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4a99c1691af96",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Features\n",
    "## We will find the effect of each feature on the movie’s success score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b672fa4650b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.384111300Z",
     "start_time": "2023-11-17T13:20:28.127221600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "processed_directory = ADDITIONAL_FOLDER\n",
    "try:\n",
    "    os.makedirs(processed_directory)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878133d338b964f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.385110500Z",
     "start_time": "2023-11-17T13:20:28.143027400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "merged = movies.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560c238b5c9a4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.386112200Z",
     "start_time": "2023-11-17T13:20:28.172202600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "merged[\"countries\"].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65baf37ca6874e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.386112200Z",
     "start_time": "2023-11-17T13:20:28.187206200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for key in ast.literal_eval(merged[\"countries\"][5]).keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ffc13903352f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.437136200Z",
     "start_time": "2023-11-17T13:20:28.202531200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# country dataframe & dictionary\n",
    "try:\n",
    "    movie_countries     = pd.read_csv(processed_directory + \"/movie_countries.csv\")\n",
    "    country_dictionary  = pickle.load(open(processed_directory + \"/country_dictionary.pickle\", \"rb\"))\n",
    "\n",
    "except:\n",
    "    movie_countries = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"country_id\"])\n",
    "    country_dictionary = {}\n",
    "    for i in tqdm(range(merged.shape[0])):\n",
    "        wikiID = merged[\"wikipedia_movie_id\"][i]\n",
    "        dico = ast.literal_eval(merged[\"countries\"][i])\n",
    "        data = [{'wikipedia_movie_id': wikiID, 'country_id': key} for key in dico.keys()]\n",
    "        dataframe = pd.DataFrame(data)\n",
    "        movie_countries = pd.concat([movie_countries, dataframe], axis=0)\n",
    "        country_dictionary.update(dico)\n",
    "\n",
    "    # save the dataframe to csv\n",
    "    movie_countries.to_csv(processed_directory + \"/movie_countries.csv\", index=False)\n",
    "\n",
    "    # dump the dictionary to pickle\n",
    "    with open(processed_directory + '/country_dictionary.pickle', 'wb') as handle:\n",
    "        pickle.dump(country_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787fa30c41c9a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:28.701268700Z",
     "start_time": "2023-11-17T13:20:28.240542800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# language dataframe & dictionary\n",
    "try:\n",
    "    movie_languages     = pd.read_csv(processed_directory + \"/movie_languages.csv\")\n",
    "    language_dictionary = pickle.load(open(processed_directory + \"/language_dictionary.pickle\", \"rb\"))\n",
    "except:\n",
    "    movie_languages = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"language_id\"])\n",
    "    language_dictionary = {}\n",
    "    for i in tqdm(range(merged.shape[0])):\n",
    "        wikiID = merged[\"wikipedia_movie_id\"][i]\n",
    "        dico = ast.literal_eval(merged[\"languages\"][i])\n",
    "        data = [{'wikipedia_movie_id': wikiID, 'language_id': key} for key in dico.keys()]\n",
    "        dataframe = pd.DataFrame(data)\n",
    "        movie_languages = pd.concat([movie_languages, dataframe], axis=0)\n",
    "        language_dictionary.update(dico)\n",
    "\n",
    "    # save the dataframe to csv\n",
    "    movie_languages.to_csv(processed_directory + \"/movie_languages.csv\", index=False)\n",
    "\n",
    "    # dump the dictionary to pickle\n",
    "    with open(processed_directory + '/language_dictionary.pickle', 'wb') as handle:\n",
    "        pickle.dump(language_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b10c71389ad0b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:29.228432Z",
     "start_time": "2023-11-17T13:20:28.270133600Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    movie_genres        = pd.read_csv(processed_directory + \"/movie_genres.csv\")\n",
    "    genre_dictionary   = pickle.load(open(processed_directory + \"/genre_dictionary.pickle\", \"rb\"))\n",
    "except:\n",
    "    movie_genres = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"genre_id\"])\n",
    "    genre_dictionary = {}\n",
    "    for i in tqdm(range(merged.shape[0])):\n",
    "        wiki_id = movies[\"wikipedia_movie_id\"][i]\n",
    "        dico = ast.literal_eval(merged[\"genres\"][i])\n",
    "        data = [{'wikipedia_movie_id': wiki_id, 'genre_id': key} for key in dico.keys()]\n",
    "        dataframe = pd.DataFrame(data)\n",
    "        movie_genres = pd.concat([movie_genres, dataframe], axis=0)\n",
    "        genre_dictionary.update(dico)\n",
    "    \n",
    "    # save the dataframe to csv\n",
    "    movie_genres.to_csv(processed_directory + \"/movie_genres.csv\", index=False)\n",
    "\n",
    "    # dump the dictionary to pickle\n",
    "    with open(processed_directory + '/genre_dictionary.pickle', 'wb') as handle:\n",
    "        pickle.dump(genre_dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663d8b97a720b1f",
   "metadata": {},
   "source": [
    "We can now start by properly merging in the two new dataframes we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52de12470a467fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:29.607598200Z",
     "start_time": "2023-11-17T13:20:28.332954300Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_languages    = pd.merge(merged, movie_languages, on=\"wikipedia_movie_id\", how=\"inner\")\n",
    "merged_countries    = pd.merge(merged_languages, movie_countries, on=\"wikipedia_movie_id\", how=\"inner\")\n",
    "merged_all          = pd.merge(merged_countries, movie_genres, on=\"wikipedia_movie_id\", how=\"inner\")\n",
    "merged_all.drop(columns=[\"languages\", \"countries\", \"genres\"], inplace=True)\n",
    "merged_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f857f9fb2e2cbba",
   "metadata": {},
   "source": [
    "We can also replace the language and country ids by their corresponding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fb53db0e46ecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:32.423754300Z",
     "start_time": "2023-11-17T13:20:28.535995300Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_all[\"language_id\"].replace(language_dictionary, inplace=True)\n",
    "merged_all[\"country_id\"].replace(country_dictionary, inplace=True)\n",
    "merged_all[\"genre_id\"].replace(genre_dictionary, inplace=True)\n",
    "\n",
    "#rename language_id as language, country_id as country and genre_id as genre\n",
    "merged_all.rename(columns={\"language_id\": \"language\", \"country_id\": \"country\", \"genre_id\": \"genre\"}, inplace=True)\n",
    "merged_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28509352abea3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:33.066732700Z",
     "start_time": "2023-11-17T13:20:32.415366700Z"
    }
   },
   "outputs": [],
   "source": [
    "inflation_group = merged_all.groupby([\"country\"]).agg({\"adjusted_revenue\": \"sum\"})\n",
    "inflation_group.sort_values(by=\"adjusted_revenue\", ascending=False).head(20).plot(kind=\"bar\", title=\"Total revenue per country adjusted for inflation\", xlabel=\"Country\", ylabel=\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc664b344382853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:33.191426900Z",
     "start_time": "2023-11-17T13:20:33.024951200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Top 20 countries that have generated the most total revenue\n",
    "top_20_countries = inflation_group.sort_values(by=\"adjusted_revenue\", ascending=False).head(20).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00230058ea9c1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:33.711648100Z",
     "start_time": "2023-11-17T13:20:33.043626700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plotting the average revenue per language adjusted for inflation\n",
    "inflation_language_group = merged_all.groupby([\"language\"]).agg({\"adjusted_revenue\": \"sum\"})\n",
    "inflation_language_group.sort_values(by=\"adjusted_revenue\", ascending=False).head(20).plot(kind=\"bar\", title=\"Total revenue per language adjusted for inflation\", xlabel=\"Language\", ylabel=\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930684ec6f9d862",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Following the evolution over the years of the 20 countries that have generated the most total revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710828c13b40bbf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:33.849117300Z",
     "start_time": "2023-11-17T13:20:33.701570400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "top_20_merged   = merged_all[merged_all[\"country\"].isin(top_20_countries)]\n",
    "top_20_merged.dropna(subset=[\"adjusted_revenue\"], inplace=True)\n",
    "top_20_grouped  = top_20_merged.groupby([\"year\", \"country\"])\n",
    "top_20_sum      = top_20_grouped.agg({\"adjusted_revenue\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51dcca9ae5de80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:39.799032400Z",
     "start_time": "2023-11-17T13:20:33.812120300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ax = top_20_sum.unstack().plot(kind='bar', stacked=True, title=\"Total revenue per country adjusted for inflation\", xlabel=\"year\", ylabel=\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\", figsize=(20,10))\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b237ce122d5ed",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We clearly see that globally, the movie industry has been making more money over the years. Though we see a drop during the years that follow 2008 which might be a consequence of the subprime's crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d93061ece8ab36",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The US dominance remains present even when looking at year-by-year movies revenue adjusted for inflation. The US has been the most productive country in terms of movies production and revenue generation for the past 100 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbddf79a79025",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Revenue analysis over the length of a movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae5ce77fa11168",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Aboout 35 movies have a runtime that is longer that 500 minutes, which does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371030eb8adc461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:39.816777400Z",
     "start_time": "2023-11-17T13:20:39.797656900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "merged_all[merged_all[\"runtime\"]>=500][\"runtime\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36b960feb8d846",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can remove these outliers and perform a `pearson correlation` test between the `movie_runtime` and its `adjusted_revenue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f26b7a14a12f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:40.114470800Z",
     "start_time": "2023-11-17T13:20:39.814267Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "merged_with_inflation_no_outlier    = merged_all[merged_all[\"runtime\"]<500]\n",
    "non_nan_merged_with_inflation       = merged_with_inflation_no_outlier[~merged_with_inflation_no_outlier[\"adjusted_revenue\"].isna()]\n",
    "non_nan_merged_with_inflation       = non_nan_merged_with_inflation[~non_nan_merged_with_inflation[\"runtime\"].isna()]\n",
    "stats.pearsonr(non_nan_merged_with_inflation[\"runtime\"], non_nan_merged_with_inflation[\"adjusted_revenue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e055e8d12df61",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The p_value is low enough, we can trust the value of Pearson's coefficient `r=0.21` which indicates a weak positive correlation between `runtime` of a movie and its `adjusted_revenue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943ab94468a3b18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:58.293140400Z",
     "start_time": "2023-11-17T13:20:39.906441100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plotting revenue vs runtime with a linear regression\n",
    "sns.lmplot(x=\"runtime\", y=\"adjusted_revenue\", data=non_nan_merged_with_inflation)\n",
    "plt.xlabel(\"Runtime in minutes\")\n",
    "plt.ylabel(\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\")\n",
    "plt.title(\"Revenue vs Runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdc13a26c99d08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:58.438608400Z",
     "start_time": "2023-11-17T13:20:58.283670200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "non_nan_merged_with_inflation[\"runtime\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b0f11e59c1d56",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Augmented data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54027f67bfcdcc0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Plotting the 20 producers that have generated the most revenue over their carreer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c05d2f682f0075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:20:58.962344Z",
     "start_time": "2023-11-17T13:20:58.310669900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scraped_grouped = merged_all.groupby([\"producer\"]).agg({\"adjusted_revenue\": \"sum\"})\n",
    "scraped_grouped.sort_values(by=\"adjusted_revenue\", ascending=False).head(20).plot(kind=\"bar\", title=\"Total revenue per producer adjusted for inflation\\n for top 20 generating producers\", xlabel=\"Producer\", ylabel=\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5121e3f6f813c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "`Adjusted Revenue` and `Rating` correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f8c136ead296b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.133763600Z",
     "start_time": "2023-11-17T13:20:58.955277500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot revenue adjusted against rating score\n",
    "merged_all.plot(kind=\"scatter\", x=\"rating_score\", y=\"adjusted_revenue\", title=\"Revenue vs Rating\", xlabel=\"Rating\", ylabel=\"Equivalent revenue in USD adjusted \\n for inflation with 1$ reference of 2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53521b9d1c5b38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.289351Z",
     "start_time": "2023-11-17T13:21:00.117630400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "non_nan_scraped_merge = merged_all[~merged_all[\"rating_score\"].isna()]\n",
    "non_nan_scraped_merge = non_nan_scraped_merge[~non_nan_scraped_merge[\"adjusted_revenue\"].isna()]\n",
    "stats.pearsonr(non_nan_scraped_merge[\"rating_score\"], non_nan_scraped_merge[\"adjusted_revenue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be06a1f8fce3918a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The p_value is low enough, we can trust the value of Pearson's coefficient `r=0.14` which indicates a weak positive correlation between `rating score` of a movie and its `adjusted revenue`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5c3da87006c80",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4) Movie budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b753bc5b84ffcc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5) Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1f6ed0b31e726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.290353200Z",
     "start_time": "2023-11-17T13:21:00.193180900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_genres = movies.copy(deep=True)\n",
    "movies_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272ce295bf25200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.293350600Z",
     "start_time": "2023-11-17T13:21:00.226693700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_json_values(df, column):\n",
    "    \"\"\"\n",
    "    This function takes a dataframe and a column name containing JSON strings.\n",
    "    It parses the JSON and preserves the content/values as a list of lists.\n",
    "    \"\"\"\n",
    "    # Extract the values from each JSON object and append to a list\n",
    "    values_list = []\n",
    "    for json_str in df[column]:\n",
    "        try:\n",
    "            # Parse the JSON data\n",
    "            json_data = json.loads(json_str)\n",
    "            # Extract values and append to the list\n",
    "            values_list.append(list(json_data.values()))\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON is not valid, append a None or handle it as needed\n",
    "            values_list.append(None)\n",
    "            \n",
    "    return values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba083c5590391cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.594185400Z",
     "start_time": "2023-11-17T13:21:00.300362800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_genres['genres'] = extract_json_values(movies_genres, \"genres\")\n",
    "movies_genres.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87b769be148505",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let’s see how many movies there are per gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f3970887777b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.595206100Z",
     "start_time": "2023-11-17T13:21:00.369947300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'movies'\n",
    "# Replace 'YourColumnName' with the actual column name in your DataFrame\n",
    "genres = movies_genres['genres']\n",
    "\n",
    "# Create an empty dictionary to store genre counts\n",
    "genres_counts = {}\n",
    "\n",
    "# Iterate through each list of genres in the 'genres' column\n",
    "for genres_list in genres:\n",
    "    # Update the counts in the dictionary\n",
    "    for genre in genres_list:\n",
    "        genres_counts[genre] = genres_counts.get(genre, 0) + 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better visualization\n",
    "genres_counts_df = pd.DataFrame(list(genres_counts.items()), columns=['Genre', 'Count'])\n",
    "\n",
    "# Sort the DataFrame by the count in descending order\n",
    "genres_counts_df = genres_counts_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "genres_counts_df.head(362)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1852211fa80942",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see above, there are many genres who are very similar (technically the same but differ in terms of expression): (Comedy = Comedy film = Comedy, …), and some of them are outliers (very few movies per genre). Moreover, the 16 most popular genres are: \n",
    "`Action`, `Adventure`, `Animation`, `Comedy`, `Crime`, `Drama`, `Family`, `Fantasy`, `Period Piece`, `Horror`, `Musical`, `Romance`, `Science Fiction`, `Sport`, `Thriller and War film`. \n",
    "Therefore, we regroup all the movies into the 16 genres above (the movies who have the same genre or a very similar one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c973573c6b633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.597185100Z",
     "start_time": "2023-11-17T13:21:00.434954400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame with genre counts is named 'genre_counts_df'\n",
    "# Replace 'YourColumnName' with the actual column name in your DataFrame\n",
    "genres_column = genres_counts_df['Genre']\n",
    "\n",
    "# Define the genres of interest\n",
    "primary_genres = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family', \n",
    "                   'Fantasy', 'Period piece', 'Horror', 'Musical', 'Romance', 'Science Fiction', \n",
    "                   'Sport', 'Thriller', 'War film']\n",
    "\n",
    "# Create a dictionary to store the grouped genres and counts\n",
    "grouped_genre_data = {'Primary Genre': [], 'Grouped Genres': [], 'Count': []}\n",
    "\n",
    "# Group genres based on keywords\n",
    "for primary_genre in primary_genres:\n",
    "    # Find genres that contain the keyword\n",
    "    grouped_genres = [genre for genre in genres_column if primary_genre.lower() in genre.lower()]\n",
    "    \n",
    "    # Sum the counts for the grouped genres\n",
    "    grouped_count = genres_counts_df[genres_counts_df['Genre'].isin(grouped_genres)]['Count'].sum()\n",
    "    \n",
    "    # Add data to the dictionary\n",
    "    grouped_genre_data['Primary Genre'].append(primary_genre)\n",
    "    grouped_genre_data['Grouped Genres'].append(', '.join(grouped_genres))\n",
    "    grouped_genre_data['Count'].append(grouped_count)\n",
    "\n",
    "# Create a new DataFrame from the grouped data dictionary\n",
    "grouped_genre_counts_df = pd.DataFrame(grouped_genre_data)\n",
    "\n",
    "# Reorder the DataFrame\n",
    "grouped_genre_counts_df = grouped_genre_counts_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "grouped_genre_counts_df.head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16c42a08f160b8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will create a new column called `Generic Genre`, which is the same as the genres column but keeping only the 16 most popular genres, and removing the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def48a537e8ad54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:00.706334800Z",
     "start_time": "2023-11-17T13:21:00.475472100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_genres['Generic Genre'] = np.nan\n",
    "movies_genres.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383013b72a55bcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:01.604927900Z",
     "start_time": "2023-11-17T13:21:00.522173200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'movies' is your DataFrame\n",
    "# If not, you need to define or read it before this point\n",
    "\n",
    "# List of genres to create tables for\n",
    "genres_list = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "               'Fantasy', 'Period piece', 'Horror', 'Musical', 'Romance', 'Science Fiction',\n",
    "               'Sport', 'Thriller', 'War film']\n",
    "\n",
    "# Define a function to get the list of genres for a movie\n",
    "def get_genre_list(movie_genres):\n",
    "    return [genre for genre in genres_list if any(genre.lower() in element.lower().strip() for element in movie_genres)]\n",
    "\n",
    "# Apply the function to create the 'Generic Genre' column\n",
    "movies_genres['Generic Genre'] = movies_genres['genres'].apply(get_genre_list)\n",
    "\n",
    "movies_genres.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b7efa5c84d2aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will now create a dictionary consisting of 16 different dataframes (1 for each genre). Each one of these dataframes corresponds to all the movie of the specific genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e4d5a8b9dda4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:01.616438800Z",
     "start_time": "2023-11-17T13:21:01.167619300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "genres_list = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family',\n",
    "               'Fantasy', 'Period piece', 'Horror', 'Musical', 'Romance', 'Science Fiction',\n",
    "               'Sport', 'Thriller', 'War film']\n",
    "#for genre in genres_list: \n",
    "#    genre_movies = movies[movies['Generic Genre'].apply(lambda x: genre in x if x is not None else False)]\n",
    "#   genre_movies.head(10)\n",
    "\n",
    "genre_dataframes = {}\n",
    "\n",
    "for genre in genres_list:\n",
    "    genre_movies = movies_genres[movies_genres['Generic Genre'].apply(lambda x: genre in x if x is not None else False)]\n",
    "    genre_dataframes[genre] = genre_movies\n",
    "\n",
    "# Access specific genre DataFrames using genre names\n",
    "action_movies = genre_dataframes['Action']\n",
    "adventure_movies = genre_dataframes['Adventure']\n",
    "animation_movies = genre_dataframes['Animation']\n",
    "comedy_movies = genre_dataframes['Comedy']\n",
    "crime_movies = genre_dataframes['Crime']\n",
    "drama_movies = genre_dataframes['Drama']\n",
    "family_movies = genre_dataframes['Family']\n",
    "fantasy_movies = genre_dataframes['Fantasy']\n",
    "period_piece_movies = genre_dataframes['Period piece']\n",
    "horror_movies = genre_dataframes['Horror']\n",
    "musical_movies = genre_dataframes['Musical']\n",
    "romance_movies = genre_dataframes['Romance']\n",
    "scifi_movies = genre_dataframes['Science Fiction']\n",
    "sport_movies = genre_dataframes['Sport']\n",
    "thriller_movies = genre_dataframes['Thriller']\n",
    "war_movies = genre_dataframes['War film']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b0acf1484b7d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let’s now do a distribution of the movie score per genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3812d26d289f088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.510521100Z",
     "start_time": "2023-11-17T13:21:01.299457700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'genre_dataframes' is the dictionary of genre-specific DataFrames\n",
    "\n",
    "# Create a 4x4 subplot grid for all genres except 'Action'\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the genres (excluding 'Action') and corresponding DataFrames\n",
    "for i, (genre, genre_df) in enumerate(genre_dataframes.items()):\n",
    "    if genre != 'Action':\n",
    "        plt.xscale(\"log\")\n",
    "        # Plot the distribution of movie revenue for each genre\n",
    "        sns.histplot(ax=axes[i], data=genre_df, x='movie_score', kde=True, bins=20, color='C{}'.format(i))\n",
    "        axes[i].set_title(genre)\n",
    "        axes[i].set_xlabel('Movie Score')\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "# Plot 'Action' genre in the middle of the 5th row, 16th subplot\n",
    "action_df = genre_dataframes['Action']\n",
    "# Set x-axis to log scale for the 'Action' subplot\n",
    "axes[0].set_xscale('log')\n",
    "sns.histplot(ax=axes[0], data=action_df, x='movie_score', kde=True, bins=20)\n",
    "axes[0].set_title('Action')\n",
    "axes[0].set_xlabel('Movie Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159de89fe031225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.933480500Z",
     "start_time": "2023-11-17T13:21:11.498570400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Revenue statistics per genre\n",
    "# Create empty dictionaries to store statistics for each genre\n",
    "genre_stats = {'Mean': {}, 'Median': {}, 'Maximum': {}, 'Minimum': {}}\n",
    "\n",
    "# Iterate through the genres and corresponding DataFrames\n",
    "for genre, genre_df in genre_dataframes.items():\n",
    "    mean_score = genre_df['movie_score'].mean()\n",
    "    median_score = genre_df['movie_score'].median()\n",
    "    max_score = genre_df['movie_score'].max()\n",
    "    min_score = genre_df['movie_score'].min()\n",
    "\n",
    "    # Store the results in the dictionaries\n",
    "    genre_stats['Mean'][genre] = mean_score\n",
    "    genre_stats['Median'][genre] = median_score\n",
    "    genre_stats['Maximum'][genre] = max_score\n",
    "    genre_stats['Minimum'][genre] = min_score\n",
    "\n",
    "# For easy displays, convert the dictionaries to Pandas DataFrames \n",
    "mean_df = pd.DataFrame(genre_stats['Mean'], index=['Mean Movie Score'])\n",
    "median_df = pd.DataFrame(genre_stats['Median'], index=['Median Movie Score'])\n",
    "max_df = pd.DataFrame(genre_stats['Maximum'], index=['Maximum Movie Score'])\n",
    "min_df = pd.DataFrame(genre_stats['Minimum'], index=['Minimum Movie Score'])\n",
    "\n",
    "# Concatenate the DataFrames along the columns to create a general DataFrame\n",
    "general_stats_df = pd.concat([mean_df, median_df, max_df, min_df])\n",
    "\n",
    "# Display the summary DataFrame\n",
    "general_stats_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd2de8280ffbb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.933480500Z",
     "start_time": "2023-11-17T13:21:11.530162800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Identify which genre has the highest and lowest values for each statistic\n",
    "# Mean \n",
    "highest_mean_genre = general_stats_df.loc['Mean Movie Score'].idxmax()\n",
    "lowest_mean_genre = general_stats_df.loc['Mean Movie Score'].idxmin()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Highest Mean Movie Score Genre: {highest_mean_genre}\")\n",
    "print(f\"Lowest Mean Movie Score Genre: {lowest_mean_genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7b26e67a825dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.934482400Z",
     "start_time": "2023-11-17T13:21:11.545030Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Median\n",
    "highest_median_genre = general_stats_df.loc['Median Movie Score'].idxmax()\n",
    "lowest_median_genre = general_stats_df.loc['Median Movie Score'].idxmin()\n",
    "\n",
    "print(f\"Highest Median Movie Score Genre: {highest_median_genre}\")\n",
    "print(f\"Lowest Median Movie Score Genre: {lowest_median_genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdab05b7b7d9e46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.934482400Z",
     "start_time": "2023-11-17T13:21:11.561553200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Max\n",
    "highest_max_genre = general_stats_df.loc['Maximum Movie Score'].idxmax()\n",
    "lowest_max_genre = general_stats_df.loc['Maximum Movie Score'].idxmin()\n",
    "\n",
    "print(f\"Highest Maximum Movie Score Genre: {highest_max_genre}\")\n",
    "print(f\"Lowest Maximum Movie Score Genre: {lowest_max_genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c80c2c2b7e15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:11.934482400Z",
     "start_time": "2023-11-17T13:21:11.574550400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Min\n",
    "highest_min_genre = general_stats_df.loc['Minimum Movie Score'].idxmax()\n",
    "lowest_min_genre = general_stats_df.loc['Minimum Movie Score'].idxmin()\n",
    "\n",
    "print(f\"Highest Minimum Movie Score Genre: {highest_min_genre}\")\n",
    "print(f\"Lowest Minimum Movie Score Genre: {lowest_min_genre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8dfec35603996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:12.348906600Z",
     "start_time": "2023-11-17T13:21:11.594302800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a list to store DataFrames for box plots\n",
    "boxplot_genre_dfs = []\n",
    "\n",
    "# Iterate through the genres and corresponding DataFrames\n",
    "for genre, genre_df in genre_dataframes.items():\n",
    "    # Use .copy() to create a copy and add a 'Genre' column\n",
    "    genre_df_copy = genre_df.copy()\n",
    "    genre_df_copy['Genre'] = genre\n",
    "    boxplot_genre_dfs.append(genre_df_copy[['movie_score', 'Genre']])\n",
    "\n",
    "\n",
    "# Concatenate the DataFrames along the rows for box plot\n",
    "boxplot_concat = pd.concat(boxplot_genre_dfs)\n",
    "\n",
    "# Create a box plot using Seaborn\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(x='Genre', y='movie_score', data=boxplot_concat, showfliers=False)\n",
    "plt.title('Box Plot of Movie Score for Each Genre')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Movie Score')\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad69eb11719bc86",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will now find some statistics for each genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5708ea2dee00d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:12.348906600Z",
     "start_time": "2023-11-17T13:21:12.140701700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "genre_stats = {}\n",
    "\n",
    "for genre, genre_df in genre_dataframes.items():\n",
    "    genre_stats[genre] = genre_df['movie_score'].describe()\n",
    "\n",
    "print(genre_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044aca95df8d24a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 6) Inclusivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69609994-3c47-479c-be00-129ca5817d06",
   "metadata": {},
   "source": [
    "Let’s begin by retrieving actors’ information’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec0e761e598fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:13.732837600Z",
     "start_time": "2023-11-17T13:21:12.202633700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "col_names = ['wikipedia_movie_id', 'freebase_movie_id', 'release_date', 'character_name', 'date_of_birth', 'gender', 'height', 'ethnicity_id', 'name', 'age_at_release', 'freebase_character_map_id', 'freebase_character_id', 'freebase_actor_id']\n",
    "\n",
    "characters = pd.read_csv(DATA_FOLDER + 'character.metadata.tsv', sep='\\t', header=None,  names=col_names)\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825764afd227eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:14.681168300Z",
     "start_time": "2023-11-17T13:21:13.751723600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors = characters[['date_of_birth',\n",
    "    'gender',\n",
    "    'height',\n",
    "    'ethnicity_id',\n",
    "    'name',\n",
    "    'freebase_actor_id']]\n",
    "non_duped_actors = actors.drop_duplicates()\n",
    "non_duped_actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd50fa70e9e6051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:14.830160700Z",
     "start_time": "2023-11-17T13:21:14.168126800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Remove actors with non given name\n",
    "named_actors = non_duped_actors[~non_duped_actors['name'].isna() ]\n",
    "named_actors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8242abc-2272-46ba-b26d-9599ad3ad7fb",
   "metadata": {},
   "source": [
    "To look at the data, we now find out the proportion of ethnicity ids presence between actors accros all films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d606e45df9ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:15.301351600Z",
     "start_time": "2023-11-17T13:21:14.263176Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors_in_movie_eth = characters[['name','freebase_actor_id','wikipedia_movie_id','ethnicity_id']].drop_duplicates()\n",
    "actors_in_movie_eth = actors_in_movie_eth[~actors_in_movie_eth['name'].isna()].drop(columns=['name'])\n",
    "actors_in_movie_eth_counts = actors_in_movie_eth.groupby('freebase_actor_id').count().rename(columns = {'wikipedia_movie_id': 'Movie count', 'ethnicity_id': 'Number of Ethnicity tags'})\n",
    "actors_in_movie_eth_counts = actors_in_movie_eth_counts[actors_in_movie_eth_counts['Movie count'] >= 1]\n",
    "print(actors_in_movie_eth_counts[actors_in_movie_eth_counts['Number of Ethnicity tags']==0].sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47909a5646c4d7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:15.302344800Z",
     "start_time": "2023-11-17T13:21:14.876163200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors_with_eth_tags_count = actors_in_movie_eth_counts[actors_in_movie_eth_counts['Number of Ethnicity tags']!= 0]\n",
    "print(actors_with_eth_tags_count.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1467a6420f735",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Below are all the actors with tags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db31983b1f4354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:15.468546900Z",
     "start_time": "2023-11-17T13:21:14.892983400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actor_eth = characters[['name','freebase_actor_id','ethnicity_id']].drop_duplicates()\n",
    "actor_eth = actor_eth[~actor_eth['name'].isna()].drop(columns=['name'])\n",
    "actor_eth\n",
    "actor_eth[~actor_eth['ethnicity_id'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485aac4918a86a51",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The below graphs shows the distribution of ethnicity tags and actors existance in film:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f38792fc87348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:17.096887500Z",
     "start_time": "2023-11-17T13:21:15.240986800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors_in_movie_eth = characters[['name','freebase_actor_id','wikipedia_movie_id','ethnicity_id']].drop_duplicates()\n",
    "actors_in_movie_eth = actors_in_movie_eth[~actors_in_movie_eth['name'].isna()].drop(columns=['name'])\n",
    "actors_in_movie_eth.groupby('wikipedia_movie_id').count().describe()\n",
    "actors_in_movie_eth.groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ca7fe-e1d5-4f2c-ad70-8360688b9819",
   "metadata": {},
   "source": [
    "Below is the movie's distinctive ethnicity tag repartitions considering NaN values:    \n",
    "Note that per movie we consider each actor once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e9725133f4986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:17.642649Z",
     "start_time": "2023-11-17T13:21:16.311804100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eth_in_movie_eth = characters[['name','freebase_actor_id','wikipedia_movie_id','ethnicity_id']].drop_duplicates()\n",
    "eth_in_movie_eth = eth_in_movie_eth[~eth_in_movie_eth['name'].isna()].drop(columns=['name', 'freebase_actor_id'])\n",
    "eth_in_movie_eth = eth_in_movie_eth.drop_duplicates()\n",
    "eth_in_movie_eth.groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3561e0-ab17-48d5-aa41-6601d3a1d42e",
   "metadata": {},
   "source": [
    "We start our study on ethnicity by selecting movies that had sufficient information about the ethnicity in their cast. This leads us to define a certain threshold on the number of actors' ethnicity tags per film, and only consider the movies that have a number of tags larger than this threshold.   \n",
    "On these movies, we may now compute the number of different tags of each movie and plotting histograms of the new data.\n",
    "The movies which have a number of different tags larger than our defined threshold are considered as inclusive movies, while the others are considered as non-inclusive.    \n",
    "The value of the threshold may be adapted in the future to see how it affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45218af3575ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:17.642649Z",
     "start_time": "2023-11-17T13:21:17.111665Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "thresh = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbf0d20e6a6a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.377709900Z",
     "start_time": "2023-11-17T13:21:17.128359300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eth_in_movie_eth = characters[['name','freebase_actor_id','wikipedia_movie_id','ethnicity_id']].drop_duplicates()\n",
    "eth_in_movie_eth = eth_in_movie_eth[~eth_in_movie_eth['name'].isna()].drop(columns=['name', 'freebase_actor_id'])\n",
    "\n",
    "eth_in_movie_eth = eth_in_movie_eth[~eth_in_movie_eth['ethnicity_id'].isna()]\n",
    "count_by_movie_id = eth_in_movie_eth.groupby('wikipedia_movie_id').count()\n",
    "\n",
    "#Film with infos on 3 ethnicities\n",
    "filtered_data = eth_in_movie_eth[eth_in_movie_eth['wikipedia_movie_id'].isin(count_by_movie_id[count_by_movie_id >= thresh].dropna().index)]\n",
    "#Films with possible duplicates of these 3 ethnicities ie the movies for which we have sufficient data to determine whether they are inclusive or not.\n",
    "filtered_data.groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,thresh+3])\n",
    "\n",
    "#Films with at least distinct 3 ethnicities among these movies for which we have sufficient data\n",
    "filtered_data.drop_duplicates().groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,thresh+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1619c-0cf5-4c10-8d2d-186c8fdcc2b2",
   "metadata": {},
   "source": [
    "But before starting analysing the data, we need to match the given ethnicity ids with their respective values from freebase and wikidata      \n",
    "Below is the scraping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9eced63071869b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.378705200Z",
     "start_time": "2023-11-17T13:21:18.223850800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eth_indexes = actors['ethnicity_id'].value_counts().index\n",
    "eth_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3ee5d435fb22e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.378705200Z",
     "start_time": "2023-11-17T13:21:18.251790100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors['ethnicity_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d10adc5c78e816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.382718600Z",
     "start_time": "2023-11-17T13:21:18.282239400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors[~actors['ethnicity_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa460378ce49cf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.382718600Z",
     "start_time": "2023-11-17T13:21:18.332398Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikidata_id(freebase_id):\n",
    "    \"\"\"\n",
    "    Function fetching from the wikidata_id of the given freebase_id\n",
    "    \"\"\"\n",
    "    # Wikidata SPARQL endpoint\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # SPARQL query to get the Wikidata ID using the Freebase ID\n",
    "    query = f'''\n",
    "    SELECT ?item WHERE {{\n",
    "        ?item wdt:P646 \"{freebase_id}\".\n",
    "    }}\n",
    "    '''\n",
    "    \n",
    "    # Send the request to the endpoint\n",
    "    response = requests.get(endpoint_url, params={'query': query, 'format': 'json'})\n",
    "    while response.status_code == 429:\n",
    "        time.sleep(2.5)\n",
    "        print('sleeping')\n",
    "        response = requests.get(endpoint_url, params={'query': query, 'format': 'json'})\n",
    "        print('Woke up !')\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract the Wikidata ID from the response\n",
    "    results = data.get(\"results\", {}).get(\"bindings\", [])\n",
    "    if results:\n",
    "        wikidata_id = results[0].get(\"item\", {}).get(\"value\", \"\")\n",
    "        # Extract the ID from the full URI\n",
    "        wikidata_id = wikidata_id.split('/')[-1]\n",
    "        return wikidata_id\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc203ee7af7916a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:18.395711Z",
     "start_time": "2023-11-17T13:21:18.349399500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ethnicity_info(wikidata_id):\n",
    "    \"\"\"\n",
    "    Function fetching the ethnicity information using the wikidata_id\n",
    "    \"\"\"\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f'''\n",
    "    SELECT ?property ?propertyLabel ?value ?valueLabel WHERE {{\n",
    "        wd:{wikidata_id} ?p ?statement .\n",
    "        ?statement ?ps ?value .\n",
    "        \n",
    "        ?property wikibase:claim ?p.\n",
    "        ?property wikibase:statementProperty ?ps.\n",
    "        \n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    ORDER BY ?property ?value\n",
    "    '''\n",
    "    \n",
    "    # Send the request to the endpoint\n",
    "    response = requests.get(endpoint_url, params={'query': query, 'format': 'json'})\n",
    "    while response.status_code == 429:\n",
    "        time.sleep(2.5)\n",
    "        print('sleeping')\n",
    "        response = requests.get(endpoint_url, params={'query': query, 'format': 'json'})\n",
    "        print('Woke up !')\n",
    "        \n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    # Process the data to extract labels and values\n",
    "    properties = {}\n",
    "    for item in data.get(\"results\", {}).get(\"bindings\", []):\n",
    "        prop = item.get(\"propertyLabel\", {}).get(\"value\", \"\")\n",
    "        val = item.get(\"valueLabel\", {}).get(\"value\", \"\")\n",
    "        if prop and val:\n",
    "            if prop in properties:\n",
    "                properties[prop].append(val)\n",
    "            else:\n",
    "                properties[prop] = [val]\n",
    "        \n",
    "    return properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f106638128fcdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.541265400Z",
     "start_time": "2023-11-17T13:21:18.361399900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock example :\n",
    "freebase_id =  '/m/043_z22'\n",
    "wikidata_id = get_wikidata_id(freebase_id)\n",
    "print(wikidata_id)\n",
    "if wikidata_id:\n",
    "    ethnicity_info = get_ethnicity_info(wikidata_id)\n",
    "    print(ethnicity_info['instance of'])\n",
    "else:\n",
    "    print(\"Wikidata ID not found for the given Freebase ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1ade371467d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.669028600Z",
     "start_time": "2023-11-17T13:21:19.539001800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def scrape_ethnicity_infos(eth_indexes):\n",
    "    \"\"\"\n",
    "    Function scraping for a list of ethnicity values using a list of ethnicity freebase id\n",
    "    \"\"\"\n",
    "    free_ids = []\n",
    "    wiki_ids = []\n",
    "    eth_infos = []\n",
    "    for idx, act_eth in enumerate(eth_indexes):\n",
    "            if idx %10 == 0:\n",
    "                print(\"Random sleep\", idx)\n",
    "                time.sleep(2.5)\n",
    "                print('Woke up !')\n",
    "                \n",
    "            freebase_id =  act_eth\n",
    "            wikidata_id = get_wikidata_id(freebase_id)\n",
    "            \n",
    "            free_ids.append(freebase_id)\n",
    "            wiki_ids.append(wikidata_id)\n",
    "            if wikidata_id:\n",
    "                ethnicity_info = get_ethnicity_info(wikidata_id)\n",
    "                eth_infos.append(ethnicity_info)\n",
    "            else:\n",
    "                eth_infos.append(None)\n",
    "                print(\"Wikidata ID not found for the given Freebase ID.\")\n",
    "                \n",
    "    return eth_infos, wiki_ids, free_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf237b5cd534a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.684012400Z",
     "start_time": "2023-11-17T13:21:19.552490600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_ethnicity_infos(eth_indexes=None):\n",
    "    \"\"\"\n",
    "    Function used to retrieve the ethnicity values either from saved data or from scraping\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(ADDITIONAL_FOLDER + 'eth_infos.pkl', 'rb') as file:\n",
    "            eth_infos = pkl.load(file)\n",
    "        with open(ADDITIONAL_FOLDER + 'wiki_ids.pkl', 'rb') as file:\n",
    "            wiki_ids = pkl.load(file)\n",
    "        with open(ADDITIONAL_FOLDER + 'free_ids.pkl', 'rb') as file:\n",
    "            free_ids = pkl.load(file)\n",
    "        return eth_infos, wiki_ids, free_ids\n",
    "    \n",
    "    except:\n",
    "        eth_infos, wiki_ids, free_ids = scrape_ethnicity_infos(eth_indexes)\n",
    "        with open(ADDITIONAL_FOLDER + 'eth_infos.pkl', 'wb') as file:\n",
    "            pkl.dump(eth_infos, file)      \n",
    "        with open(ADDITIONAL_FOLDER + 'wiki_ids.pkl', 'wb') as file:\n",
    "            pkl.dump(wiki_ids, file)   \n",
    "        with open(ADDITIONAL_FOLDER + 'free_ids.pkl', 'wb') as file:\n",
    "            pkl.dump(free_ids, file)\n",
    "        return eth_infos, wiki_ids, free_ids      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee1337a87a207b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.688015600Z",
     "start_time": "2023-11-17T13:21:19.582081900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eth_infos, wiki_ids, free_ids = retrieve_ethnicity_infos(eth_indexes=eth_indexes)\n",
    "len(eth_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204de22bf5759a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.904399300Z",
     "start_time": "2023-11-17T13:21:19.664016200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'wiki_ids': wiki_ids,\n",
    "    'eth_infos': eth_infos,\n",
    "    'category' : np.nan\n",
    "}\n",
    "\n",
    "# Create a ethnicity information DataFrame with free_ids as the index\n",
    "ethnicity_map = pd.DataFrame(data_dict, index=free_ids)\n",
    "ethnicity_map['category'] = ethnicity_map['category'].astype('object')\n",
    "ethnicity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119fffc7ab87056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.904399300Z",
     "start_time": "2023-11-17T13:21:19.723584300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map = ethnicity_map.replace('', np.nan)\n",
    "ethnicity_map = ethnicity_map[ (~ethnicity_map['eth_infos'].isna() )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee3a623484bab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The best way to understand the data is to look at it.\n",
    "In the following subsection we are going to retrieve little by little information from the quite dirty obtained data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d447ae23e7a9a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.906759300Z",
     "start_time": "2023-11-17T13:21:19.736096400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map['eth_infos'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75289134e706b438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.906759300Z",
     "start_time": "2023-11-17T13:21:19.753429Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords_and_specific_words(text, specific_words):\n",
    "    \"\"\"\n",
    "    Function used to preprocess and clean the ethnicities values obtained by removing stop words and demanded words.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in specific_words]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1d71974add38b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.906759300Z",
     "start_time": "2023-11-17T13:21:19.773396900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EthnicityColumnProcessing(ABC):\n",
    "    \"\"\"\n",
    "    Super class used to define a generic cleaning policy of the ethnicity values.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        \"\"\"\n",
    "        Function which return a single cleaned and demanded from a dict.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_data_list(data_list):\n",
    "        \"\"\"\n",
    "        Function used to clean the hole data obtained from the ethnicity dict.\n",
    "        \"\"\"\n",
    "        if data_list is None or data_list is np.nan:\n",
    "            return np.nan\n",
    "            \n",
    "        new_data_list = []  \n",
    "        for item in data_list:\n",
    "            processed_content = item.replace('-', ' ').lower()\n",
    "            processed_content = remove_stopwords_and_specific_words(processed_content, ['people', 'peoples', 'descent'])\n",
    "            new_data_list.append(processed_content)\n",
    "        return new_data_list\n",
    "        \n",
    "    @classmethod\n",
    "    def process_dataframe(cls, df):\n",
    "        \"\"\"\n",
    "        Function used to obtain and update the ethnicity category of a dataframe with new informations.\n",
    "        \"\"\"\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['category'], list):\n",
    "                if pd.isna(row['category']).all():\n",
    "                    df.at[index, 'category'] = cls.process_data_list(cls.find_category_from_dict(row['eth_infos']))\n",
    "            elif pd.isna(row['category']):\n",
    "                df.at[index, 'category'] = cls.process_data_list(cls.find_category_from_dict(row['eth_infos']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843b1b1-7c1e-4e83-8826-59052ab5cdfc",
   "metadata": {},
   "source": [
    "We incrementally add values from the ethnicity infos `eth_info` to the ethnicity category of our dataframe `ethnicity_map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70686fbe5906094e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:19.918784500Z",
     "start_time": "2023-11-17T13:21:19.785723600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MainCategoryProcessing(EthnicityColumnProcessing):\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        category = dico.get(\"topic's main category\", np.nan)   \n",
    "        return category\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_data_list(data_list):\n",
    "        if data_list is None or data_list is np.nan:\n",
    "            return np.nan\n",
    "            \n",
    "        new_data = []  \n",
    "        for item in data_list:\n",
    "            if item.startswith(\"Category:\"):\n",
    "                content = item[len(\"Category:\"):]\n",
    "    \n",
    "                processed_content = content.replace('-', ' ').lower()\n",
    "                processed_content = remove_stopwords_and_specific_words(processed_content, ['people', 'peoples', 'descent'])\n",
    "                new_data.append(processed_content)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009e143fdf05f25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.349317500Z",
     "start_time": "2023-11-17T13:21:19.798723100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MainCategoryProcessing().process_dataframe(ethnicity_map)\n",
    "ethnicity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d76ff27bb771e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.391276700Z",
     "start_time": "2023-11-17T13:21:20.174795600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map['eth_infos'].loc['/m/03x_fq7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ebfb250fa282c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.732114100Z",
     "start_time": "2023-11-17T13:21:20.193226600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CommonCategoryProcessing(EthnicityColumnProcessing):\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        category = dico.get('Commons category', np.nan)   \n",
    "        return category\n",
    "\n",
    "CommonCategoryProcessing().process_dataframe(ethnicity_map)\n",
    "display(ethnicity_map)\n",
    "ethnicity_map.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449e00c4a5a0227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.875254500Z",
     "start_time": "2023-11-17T13:21:20.367316Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class EncyclopedyCategoryProcessing(EthnicityColumnProcessing):\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        if isinstance(dico, dict):\n",
    "            eth = dico.get('Encyclopædia Britannica Online ID', np.nan)\n",
    "            if eth is not np.nan:\n",
    "                eth = eth[0].removeprefix(\"topic/\")\n",
    "                    \n",
    "            return eth\n",
    "        else: \n",
    "            return np.nan\n",
    "        \n",
    "EncyclopedyCategoryProcessing().process_dataframe(ethnicity_map)\n",
    "ethnicity_map.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b196fd7538efff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.876255500Z",
     "start_time": "2023-11-17T13:21:20.427996100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/06mvq'].eth_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad82ff4579b96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.940082100Z",
     "start_time": "2023-11-17T13:21:20.443460700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class QuoraCategoryProcessing(EthnicityColumnProcessing):\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        category = dico.get('Quora topic ID', np.nan)   \n",
    "        return category\n",
    "        \n",
    "QuoraCategoryProcessing().process_dataframe(ethnicity_map)\n",
    "ethnicity_map.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1b06b58a58777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.941081500Z",
     "start_time": "2023-11-17T13:21:20.500470300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map[(ethnicity_map['category'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0317f40318c24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.942081600Z",
     "start_time": "2023-11-17T13:21:20.568512900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/0g96wd'].eth_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0efd21d88cc5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.942081600Z",
     "start_time": "2023-11-17T13:21:20.586919500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/02rm7_9'].eth_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fc334963dff61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.942081600Z",
     "start_time": "2023-11-17T13:21:20.603946100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CountryCategoryProcessing(EthnicityColumnProcessing):\n",
    "    @staticmethod\n",
    "    def find_category_from_dict(dico):\n",
    "        category = dico.get('country', np.nan)   \n",
    "        return category\n",
    "        \n",
    "CountryCategoryProcessing().process_dataframe(ethnicity_map)\n",
    "ethnicity_map.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184aa3f3647024f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.942081600Z",
     "start_time": "2023-11-17T13:21:20.649025600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/01hm_'].eth_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c6fe9822bf642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.943075400Z",
     "start_time": "2023-11-17T13:21:20.675424500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/047q05d'].eth_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1c237652621e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:20.960083300Z",
     "start_time": "2023-11-17T13:21:20.720862600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/03b_13l'].eth_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99274f95dfa6440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:21.066820100Z",
     "start_time": "2023-11-17T13:21:20.743286300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ethnicity_map.loc['/m/0960kn'].eth_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b54e95f58ae76c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We see now that most of the useful obtained values are in our dataframe.   \n",
    "we therefore stop the incremental dataframe update here.   \n",
    "We find now all actors with existing mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab39e841648033ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.335292300Z",
     "start_time": "2023-11-17T13:21:20.757304100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# All actors with existing mapping\n",
    "useful_eth_tags = ethnicity_map[~ethnicity_map['category'].isna()]\n",
    "useful_eth_tags\n",
    "\n",
    "actors_with_eth_tags = characters[['name','freebase_actor_id','wikipedia_movie_id','ethnicity_id']].drop_duplicates()\n",
    "actors_with_eth_tags = actors_with_eth_tags[~actors_with_eth_tags['name'].isna()].drop(columns=['name'])\n",
    "\n",
    "actors_with_eth_tags = actors_with_eth_tags[~actors_with_eth_tags['ethnicity_id'].isna()]\n",
    "actors_with_eth_categories = actors_with_eth_tags[actors_with_eth_tags['ethnicity_id'].isin(useful_eth_tags.index)]\n",
    "actors_with_eth_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c545e8d076f0d25",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We now recompute the last cell to check if any drastic changes in the data occured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339afdbc31a50d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.468135700Z",
     "start_time": "2023-11-17T13:21:21.186663500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "eth_in_movie_eth = actors_with_eth_categories.drop(columns=['freebase_actor_id'])\n",
    "\n",
    "eth_in_movie_eth = eth_in_movie_eth[~eth_in_movie_eth['ethnicity_id'].isna()]\n",
    "count_by_movie_id = eth_in_movie_eth.groupby('wikipedia_movie_id').count()\n",
    "\n",
    "#Film with infos on 3 ethnicities\n",
    "filtered_data = eth_in_movie_eth[eth_in_movie_eth['wikipedia_movie_id'].isin(count_by_movie_id[count_by_movie_id >= thresh].dropna().index)]\n",
    "\n",
    "#Films with possible duplicates of these 3 ethnicities ie the movies for which we have sufficient data to determine whether they are inclusive or not.\n",
    "filtered_data.groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,thresh+3])\n",
    "\n",
    "#Films with at least distinct 3 ethnicities among these movies for which we have sufficient data\n",
    "filtered_data.drop_duplicates().groupby('wikipedia_movie_id').count().hist(bins=20, range = [0,thresh+3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a74fd7609aeec",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We saw that no drastic change occured except for some loss of movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e091409c44f14",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now let's start our little analysis by first redefining the choosen threshold for separating non inclusive and inclusive films :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6ca2ee6cb3c5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.468135700Z",
     "start_time": "2023-11-17T13:21:21.929057Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2986a074cf02bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.468135700Z",
     "start_time": "2023-11-17T13:21:21.947330500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "film_ethnicity_difference_repartition = filtered_data.drop_duplicates().groupby('wikipedia_movie_id').count()\n",
    "low_inclusivity_films_ids = film_ethnicity_difference_repartition[film_ethnicity_difference_repartition < threshold].dropna().index\n",
    "high_inclusivity_films_ids = film_ethnicity_difference_repartition[film_ethnicity_difference_repartition >= threshold].dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4abcb6e1046992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.468135700Z",
     "start_time": "2023-11-17T13:21:21.994318500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_inclusivity = movies.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b717c36b01222d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.470133200Z",
     "start_time": "2023-11-17T13:21:22.009334900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "low_inclusivity_movies = movies_inclusivity[movies_inclusivity['wikipedia_movie_id'].isin(low_inclusivity_films_ids)]\n",
    "high_inclusivity_movies = movies_inclusivity[movies_inclusivity['wikipedia_movie_id'].isin(high_inclusivity_films_ids)]\n",
    "display(low_inclusivity_movies)\n",
    "display(high_inclusivity_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53b18b39ca666c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.471131200Z",
     "start_time": "2023-11-17T13:21:22.091178300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inclusive_good_movies = df_good_movies[df_good_movies['wikipedia_movie_id'].isin(high_inclusivity_films_ids)].copy(deep=True)\n",
    "inclusive_bad_movies = df_bad_movies[df_bad_movies['wikipedia_movie_id'].isin(high_inclusivity_films_ids)].copy(deep=True)\n",
    "exclusive_good_movies = df_good_movies[df_good_movies['wikipedia_movie_id'].isin(low_inclusivity_films_ids)].copy(deep=True)\n",
    "exclusive_bad_movies = df_bad_movies[df_bad_movies['wikipedia_movie_id'].isin(low_inclusivity_films_ids)].copy(deep=True)\n",
    "\n",
    "inc_good_len = len(inclusive_good_movies)\n",
    "inc_bad_len = len(inclusive_bad_movies)\n",
    "ex_good_len = len(exclusive_good_movies)\n",
    "ex_bad_len = len(exclusive_bad_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db493d4288d3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.471131200Z",
     "start_time": "2023-11-17T13:21:22.119279400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of good inclusive movies {inc_good_len}\")\n",
    "print(f\"Number of good exclusive movies {ex_good_len} \")\n",
    "print()\n",
    "print(f\"Number of bad inclusive movies {inc_bad_len}\")\n",
    "print(f\"Number of bad exclusive movies {ex_bad_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265fd1a3742d659b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "With a naive and somewhat incorrect analysis, we see that inclusive movies tend to be worse than their counter parts, but somehow more rewarding \n",
    "when successful. \n",
    "A deeper probabilistic analysis is therefore needed here to know if there is a significant difference between inclusive and exclusive movies success "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9803fe729256ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As a first step in this direction, lets compute the proportion of inclusive and exclusive movies that are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4d2ecbda070d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.471131200Z",
     "start_time": "2023-11-17T13:21:22.134023100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Proportion of inclusive movies that are good movies {inc_good_len / (inc_good_len + inc_bad_len)}\")\n",
    "print(f\"Proportion of exclusive movies that are good movies {ex_good_len / (ex_good_len + ex_bad_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c2ae98f76bc7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:22.471131200Z",
     "start_time": "2023-11-17T13:21:22.153141500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "exclusive_mean = low_inclusivity_movies[\"movie_score\"].mean()\n",
    "inclusive_mean = high_inclusivity_movies[\"movie_score\"].mean()\n",
    "\n",
    "ex_std = low_inclusivity_movies[\"movie_score\"].std()\n",
    "in_std = high_inclusivity_movies[\"movie_score\"].std()\n",
    "\n",
    "e_b = exclusive_mean - 1.96 * ex_std / np.sqrt(low_inclusivity_movies['movie_score'].shape[0])\n",
    "e_u = exclusive_mean + 1.96 * ex_std / np.sqrt(low_inclusivity_movies['movie_score'].shape[0])\n",
    "\n",
    "i_b = inclusive_mean - 1.96 * in_std / np.sqrt(high_inclusivity_movies['movie_score'].shape[0])\n",
    "i_u = inclusive_mean + 1.96 * in_std / np.sqrt(high_inclusivity_movies['movie_score'].shape[0])\n",
    "\n",
    "print(f\"Exclusive movies' mean movie score 95% confidence interval: [{e_b:.4f}, {e_u:.4f}]\")\n",
    "print(f\"Inclusive movies' mean movie score 95% confidence interval: [{i_b:.4f}, {i_u:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9bf86dc1f90fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:23.660404400Z",
     "start_time": "2023-11-17T13:21:22.168136Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#confid = df[~df[\"salary\"].isna()][[\"salary\", \"throws\"]]\n",
    "inc = pd.DataFrame()\n",
    "exl = pd.DataFrame()\n",
    "\n",
    "inc['movie_score'] = high_inclusivity_movies[\"movie_score\"]\n",
    "exl['movie_score'] = low_inclusivity_movies[\"movie_score\"]\n",
    "\n",
    "inc['inclusivity'] = \"Highly Inclusive\"\n",
    "exl['inclusivity'] = \"Lowly Inclusive\"\n",
    "\n",
    "confid = pd.concat([inc, exl])\n",
    "#display(confid)\n",
    "sns.pointplot(x=\"inclusivity\",\n",
    "              y=\"movie_score\",\n",
    "              data=confid,\n",
    "              errorbar=('ci', 95),\n",
    "              n_boot=10000,\n",
    "              seed=1,\n",
    "              )\n",
    "plt.title('Point plot showing confidence intervals of movie scores in function of inclusivity of the film')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86f064-d886-4927-a3b8-daee3146beae",
   "metadata": {},
   "source": [
    "This simplistic analysis therefore shows that inclusive films somewhat performs better than non-inclusive one.\n",
    "However some issues as data imbalance must be addressed further on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b1cfafcfcf72f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7) Sequels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae1e9d",
   "metadata": {},
   "source": [
    "### Firstly We fetch Sequel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deac189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data comes from https://data.world/priyankad0993/sequels\n",
    "df_sequels = pd.read_excel(DATA_FOLDER + 'sequels.xlsx')[[\"Title\", \"Release Date\", \"Year\", \"IMDb Rating\", \"Runtime (mins)\", \"Genres\", \"Num Votes\", \"Directors\", \"Movie Series\", \"Order\"]]#,\n",
    "              #dtype={'Name': str, 'Value': float}) \n",
    "\n",
    "df_sequels[\"Genres\"] = df_sequels[\"Genres\"].apply(lambda x: x.split(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb025b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the titles in lower case\n",
    "df_sequels['Title'] = df_sequels['Title'].str.lower().str.replace('[^\\w\\s]','')\n",
    "df_data['movie_title'] = df_data['movie_title'].str.lower().str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4ee7c",
   "metadata": {},
   "source": [
    "### we only want to keep those for which we have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_series = df_sequels.merge(df_data, left_on=['Title', \"Year\"], right_on=['movie_title', \"Year\"], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4719a",
   "metadata": {},
   "source": [
    "### Now that we have the sequels and prequels we need to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we \n",
    "df_movie_series[~df_movie_series[\"box_office_revenue\"].isna()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b50dfe",
   "metadata": {},
   "source": [
    "### We remove movies where we don't have data for at least two of the series\n",
    "\n",
    "Sometimes the sequel came after our dataset cutoff (November 2, 2012) so we don't have data for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b29b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dont keep movies with only one movie in that Movie Series\n",
    "df_movie_series = df_movie_series.groupby('Movie Series').filter(lambda x: len(x) >= 2)\n",
    "df_movie_series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b4459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by \"Movie Series\"\n",
    "grouped = df_movie_series.groupby('Movie Series')\n",
    "\n",
    "# Creating a DataFrame for prequels (movies with the minimum Order in each group)\n",
    "prequels_df = df_movie_series[df_movie_series['Order'] == grouped['Order'].transform('min')]\n",
    "\n",
    "# Creating a DataFrame for sequels (movies where Order is not the minimum in each group)\n",
    "sequels_df = df_movie_series[df_movie_series['Order'] != grouped['Order'].transform('min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de452b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Creating a figure with two subplots\n",
    "# Combine the data for both prequels and sequels to find the common y-axis limit\n",
    "combined_data = pd.concat([prequels_df[\"Movie Score\"], sequels_df[\"Movie Score\"]])\n",
    "y_max = combined_data.max()  # Find the maximum value to set the y-axis limit\n",
    "\n",
    "\n",
    "# Plotting boxplot for prequels\n",
    "axes[0].boxplot(prequels_df[\"Movie Score\"])\n",
    "axes[0].set_title('Prequels Boxplot')\n",
    "axes[0].set_ylabel('Movie Score (function of revenue and rating)')\n",
    "axes[0].set_ylim(-2, y_max + 2) \n",
    "\n",
    "# Plotting boxplot for sequels\n",
    "axes[1].boxplot(sequels_df[\"Movie Score\"])\n",
    "axes[1].set_title('Sequels Boxplot')\n",
    "axes[1].set_ylabel('Movie Score (function of revenue and rating)')\n",
    "axes[1].set_ylim(-2, y_max + 2) \n",
    "\n",
    "plt.tight_layout()  # Adjust layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, shapiro, levene\n",
    "\n",
    "prequel_scores = prequels_df[\"Movie Score\"]\n",
    "sequel_scores = sequels_df[\"Movie Score\"]\n",
    "\n",
    "\n",
    "# Check assumptions (normality and variance equality)\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "shapiro_sequels = shapiro(prequel_scores)\n",
    "shapiro_prequels = shapiro(sequel_scores)\n",
    "\n",
    "print(shapiro_sequels)\n",
    "print(shapiro_prequels)\n",
    "\n",
    "# Perform Levene's test for equality of variances\n",
    "levene_test = levene(sequel_scores, prequel_scores)\n",
    "print(levene_test)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(sequel_scores, prequel_scores, equal_var=True)  # Assuming equal variances\n",
    "\n",
    "print(t_stat, \" \", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject Null Hypothesis: Significant difference between sequel and prequel scores.\")\n",
    "else:\n",
    "    print(\"Failed to reject Null Hypothesis: No significant difference between sequel and prequel scores.\")\n",
    "\n",
    "# the Shapiro-Wilk tests indicate non-normality, the Levene test suggests unequal variances, \n",
    "# and the t-test suggests a significant difference in the means of the two groups. \n",
    "# These results indicate that assumptions for a t-test might not hold, and alternative approaches \n",
    "# might be necessary to compare the means of the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fc40b",
   "metadata": {},
   "source": [
    "### We look at the differences in scores between the prequel and corresponding sequels to see if in general it increases or decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping prequel scores to their series\n",
    "prequel_scores_dict = prequels_df.set_index('Movie Series')['Movie Score'].to_dict()\n",
    "\n",
    "# Function to calculate the score difference based on the series\n",
    "def score_difference(row):\n",
    "    prequel_score = prequel_scores_dict.get(row['Movie Series'], None)\n",
    "    if prequel_score is not None:\n",
    "        return row['Movie Score'] - prequel_score\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create a new column 'Score Difference'\n",
    "sequels_df['Score Difference'] = sequels_df.apply(score_difference, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff49d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequels_df[\"Score Difference\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e0967",
   "metadata": {},
   "source": [
    "#### Clearly in general, sequels are worst, but we'd like to see if the movies that did improve have something in common.\n",
    "#### In other words we are searching for a positive change from prequel to sequels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequels_that_improved = sequels_df[sequels_df[\"Score Difference\"] >= 0][[\"Title\", \"Release Date\",\"IMDb Rating\", \"Genres\", \"AdjustedRevenue\", \"Movie Score\", \"Score Difference\", \"Good Movie\"]]\n",
    "sequels_that_worstened = sequels_df[sequels_df[\"Score Difference\"] < 0][[\"Title\", \"Release Date\",\"IMDb Rating\", \"Genres\", \"AdjustedRevenue\", \"Movie Score\", \"Score Difference\", \"Good Movie\"]]\n",
    "sequels_that_improved.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1275c",
   "metadata": {},
   "source": [
    "We can see that most of these movies are Categorized as Good Movies. Lets dig deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a834d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pie charts for 'Good Movie' column in prequels and sequels DataFrames\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Choose contrasting shades within the 'vlag' palette\n",
    "colors = [sns.color_palette('vlag')[0], sns.color_palette('vlag')[-1]]\n",
    "\n",
    "keys = [\"Bad Movies\", \"Good Movies\"]\n",
    "prequel_counts = prequels_df[\"Good Movie\"].value_counts()\n",
    "sequel_counts = sequels_df[\"Good Movie\"].value_counts()\n",
    "\n",
    "# declaring exploding pie \n",
    "explode = [0.1, 0] \n",
    "# define Seaborn color palette to use\n",
    "  \n",
    "# plotting prequel Good Movie share\n",
    "axs[0].pie([prequel_counts[0], prequel_counts[1]], labels=keys, colors=colors, \n",
    "        explode=explode, autopct='%.0f%%') \n",
    "axs[0].set_title(\"Prequel share of Good and Bad Movies\")\n",
    "\n",
    "# Sequels\n",
    "axs[1].pie([sequel_counts[0], sequel_counts[1]], labels=keys, colors=colors, \n",
    "        explode=explode, autopct='%.0f%%') \n",
    "axs[1].set_title(\"Sequel share of Good and Bad Movies\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91181c5",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "We can see that much more sequels are good movies than bad movies, eventhough most of them worsten in Movie Score\n",
    "\n",
    "As if sequels are a safe way of surfing the popularity of a previous movie; however, they don't usually result in increased popularity.\n",
    "\n",
    "In addition, it makes sense that a few prequels are Bad Movies, because it is very risky to tackle a poor movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331675f",
   "metadata": {},
   "source": [
    "### Can we find a trend in the sequels that improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdf7dd",
   "metadata": {},
   "source": [
    "#### Lets see if we can find one among the Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87d754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explode the lists in the 'genres' column\n",
    "exploded_genres = sequels_that_improved.explode('Genres')\n",
    "\n",
    "# Get the count of each genre\n",
    "genre_counts = exploded_genres['Genres'].value_counts()\n",
    "genre_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lists in the 'genres' column\n",
    "exploded_genres = sequels_that_worstened.explode('Genres')\n",
    "\n",
    "# Get the count of each genre\n",
    "genre_counts = exploded_genres['Genres'].value_counts()\n",
    "genre_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca9b9c",
   "metadata": {},
   "source": [
    "Can't see any significant differences (note that there are more sequels that worstened which explains the differences in counts) and the data is too sparse to try and find slight distinctions between the two sets.\n",
    "\n",
    "Deeper analysis will be performed in P3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46629862ed25601",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 8) Actors' popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429242d5d598064d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First we need to read the actors dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdc339e194452b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:24.316774Z",
     "start_time": "2023-11-17T13:21:23.315707300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "characters_columns = ['wikipedia_movie_id', 'freebase_movie_id', 'release_date', 'character_name', 'actor_dob',\n",
    "                      'gender',\n",
    "                      'height', 'ethnicity', 'actor_name', 'actor_age_at_release', 'freebase_character_map', '0', '1']\n",
    "\n",
    "characters = pd.read_csv(DATA_FOLDER + 'character.metadata.tsv', sep='\\t', header=None, names=characters_columns,\n",
    "                         usecols=['wikipedia_movie_id', 'actor_dob', 'actor_name', 'character_name', 'release_date'])\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbdd5f5dee55b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:24.320798400Z",
     "start_time": "2023-11-17T13:21:24.155074700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_actors = movies.copy(deep=True)\n",
    "movies_actors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d9989151a933f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We don't need all the columns of movies dataset for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efbf16e2bf0bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:24.321793800Z",
     "start_time": "2023-11-17T13:21:24.204108600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movies_actors = movies_actors[\n",
    "    ['wikipedia_movie_id', 'name', 'release_date', 'movie_score', 'adjusted_revenue', 'genres', 'year']]\n",
    "movies_actors.rename(columns={'genres': 'movie_genres', 'name': 'movie_title'}, inplace=True)\n",
    "movies_actors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eabb5a15e95764",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Filtering the actors dataset to only contain actors of the movies left after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da872907ececa8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:24.464004Z",
     "start_time": "2023-11-17T13:21:24.245035500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Number of actors before filtering: ', characters.shape[0])\n",
    "characters_relevant = characters[characters['wikipedia_movie_id'].isin(movies_actors['wikipedia_movie_id'])]\n",
    "characters_relevant = characters_relevant[~characters_relevant['actor_name'].isna()]\n",
    "print('Number of actors after filtering: ', characters_relevant.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04eb52fc8c1359",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We want to define actor's popularity at the time of the movie release. For this reason we want to sum `movie_score` of movies released on previous years for which actor was part of the cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648de3f477532d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:24.748793Z",
     "start_time": "2023-11-17T13:21:24.311777100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_actors_popularity(movies_data, char_data, lookback_movies=-1):\n",
    "    popularity = []\n",
    "\n",
    "    movies_data.sort_values(by=['year'], inplace=True)\n",
    "    \n",
    "    for i, movie in tqdm(movies_data.iterrows(), total=movies_data.shape[0]):\n",
    "        actors = char_data[char_data['wikipedia_movie_id'] == movie['wikipedia_movie_id']]\n",
    "        for j, actor in actors.iterrows():\n",
    "            actor_name = actor['actor_name']\n",
    "\n",
    "            relevant_movies = char_data[char_data['actor_name'] == actor_name]['wikipedia_movie_id']\n",
    "            actor_movies = movies_data[movies_data['wikipedia_movie_id'].isin(relevant_movies)]\n",
    "            \n",
    "            if lookback_movies > 0:\n",
    "                actor_movies = actor_movies[actor_movies['year'] < movie['year']][-lookback_movies:]\n",
    "            else:\n",
    "                actor_movies = actor_movies[actor_movies['year'] < movie['year']]\n",
    "\n",
    "            popularity.append({'actor_name': actor_name, 'actor_popularity': actor_movies['movie_score'].sum(),\n",
    "                                      'year': movie['year'], 'character_name': actor['character_name'],\n",
    "                                      'movie_title': movie['movie_title'],\n",
    "                                      'wikipedia_movie_id': movie['wikipedia_movie_id'],\n",
    "                                      'movie_score': movie['movie_score']})\n",
    "\n",
    "    return pd.DataFrame(popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa6f37e10cae33",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Popularity of the actors in all the previous movies they have played in\n",
    "Here we are taking into account all the movies the actor has played in before the movie we are considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166f3ae06dd56d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:25.025231200Z",
     "start_time": "2023-11-17T13:21:24.326784800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    actors_popularity = pd.read_csv(ADDITIONAL_FOLDER + 'actors_popularity.csv')\n",
    "except:\n",
    "    actors_popularity = get_actors_popularity(movies_actors, characters_relevant)\n",
    "    actors_popularity.to_csv(ADDITIONAL_FOLDER + 'actors_popularity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f59399acaf7a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:25.189706600Z",
     "start_time": "2023-11-17T13:21:24.691258400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actors_popularity.sort_values(['actor_name', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c69c4a4975eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:25.261554300Z",
     "start_time": "2023-11-17T13:21:24.856744Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grouped_popularity = actors_popularity.groupby(['movie_title', 'movie_score']).agg(\n",
    "    sum_pop=('actor_popularity', 'sum')\n",
    ").reset_index()\n",
    "grouped_popularity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7fbc976b32c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:25.261554300Z",
     "start_time": "2023-11-17T13:21:24.910742800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stats.spearmanr(grouped_popularity['movie_score'], grouped_popularity['sum_pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2530f3056b3cb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see there is a small positive correlation between the popularity of the actors and the movie score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a5182abb2803",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Popularity of the actors in the last 5 movies they have played in\n",
    "Here we are taking into account only the last 5 movies the actor has played in before the movie we are considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f4ca08e15b4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:26.094021600Z",
     "start_time": "2023-11-17T13:21:24.926451400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    actors_popularity = pd.read_csv(ADDITIONAL_FOLDER + 'actors_popularity_5.csv')\n",
    "except:\n",
    "    actors_popularity = get_actors_popularity(movies_actors, characters_relevant, 5)\n",
    "    actors_popularity.to_csv(ADDITIONAL_FOLDER + 'actors_popularity_5.csv', index=False)\n",
    "\n",
    "grouped_popularity = actors_popularity.groupby(['movie_title', 'movie_score']).agg(\n",
    "    sum_pop=('actor_popularity', 'sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfb7eb399fb61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:26.094021600Z",
     "start_time": "2023-11-17T13:21:25.316602400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stats.spearmanr(grouped_popularity['movie_score'], grouped_popularity['sum_pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a43aa480865b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see there is a small positive correlation between the popularity of the actors and the movie score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66a1cd67a47ef",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Popularity of actors vs movie score for different genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260d1253ddfe686",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We want to see if the correlation between the popularity of actors and the movie score is different for different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb42ba6aed52ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:26.094021600Z",
     "start_time": "2023-11-17T13:21:25.331730200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "popular_actors_by_genres = movies_actors.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52488a3c5705f5c2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "First we need to separate genres for each movie making them separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8179a207dbee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:53.850302300Z",
     "start_time": "2023-11-17T13:21:25.360329Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "movie_genres = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"genre_id\"])\n",
    "genre_dictionary = {}\n",
    "for i in range(popular_actors_by_genres.shape[0]):\n",
    "    wiki_id = popular_actors_by_genres[\"wikipedia_movie_id\"].iloc[i]\n",
    "    dico = ast.literal_eval(popular_actors_by_genres[\"movie_genres\"][i])\n",
    "    data = [{'wikipedia_movie_id': wiki_id, 'genre_id': key} for key in dico.keys()]\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    movie_genres = pd.concat([movie_genres, dataframe], axis=0)\n",
    "    genre_dictionary.update(dico)\n",
    "\n",
    "movie_genres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b49a729db38fd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here is the list of all the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ace9f5d9be1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:53.903825500Z",
     "start_time": "2023-11-17T13:21:53.843065600Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "genre_dictionary.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc06ee00e43ccf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's plot the number of movies per genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6fccf6e14ec19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:54.652353600Z",
     "start_time": "2023-11-17T13:21:53.868823900Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "values = movie_genres['genre_id'].value_counts()\n",
    "values.rename(index=genre_dictionary, inplace=True)\n",
    "values[:50].plot(kind='bar')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of movies')\n",
    "plt.title('Number of movies per genre for 50 genres', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85418bdcf118c9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "As we can see there are some genres with a very small number of movies. We will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4d18bbb69d304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:56.888271900Z",
     "start_time": "2023-11-17T13:21:54.653354200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "considered_number_of_movies = 500\n",
    "\n",
    "reduced_movie_genres = movie_genres.groupby('genre_id').filter(lambda x: len(x) > considered_number_of_movies).copy(\n",
    "    deep=True)\n",
    "reduced_genre_dictionary = {key: value for key, value in genre_dictionary.items() if\n",
    "                            key in reduced_movie_genres['genre_id'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699fc7a8b251e1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:56.937390300Z",
     "start_time": "2023-11-17T13:21:56.872953200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "reduced_genre_dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b948b25c7cac3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:57.193368800Z",
     "start_time": "2023-11-17T13:21:56.893293200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    actors_popularity = pd.read_csv(ADDITIONAL_FOLDER + 'actors_popularity.csv')\n",
    "except:  \n",
    "    actors_popularity = get_actors_popularity(movies_actors, characters_relevant)\n",
    "    actors_popularity.to_csv(ADDITIONAL_FOLDER + 'actors_popularity.csv', index=False)\n",
    "    \n",
    "actors_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108484e7b50478",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We need to separate different genres into different datasets and merge them with actors popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120434e16440f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:21:59.738198400Z",
     "start_time": "2023-11-17T13:21:57.197945800Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "datasets_by_genres = []\n",
    "\n",
    "for genre in reduced_genre_dictionary:\n",
    "    genre_movies = reduced_movie_genres[reduced_movie_genres['genre_id'] == genre]\n",
    "    genre_movies = pd.merge(genre_movies, actors_popularity, on='wikipedia_movie_id')\n",
    "    datasets_by_genres.append(genre_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a83ad01d933229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:22:00.534630500Z",
     "start_time": "2023-11-17T13:21:59.742220400Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "grouped_datasets_by_genres = []\n",
    "\n",
    "for df in datasets_by_genres:\n",
    "    grouped_popularity = df.groupby(['movie_title', 'movie_score', 'genre_id']).agg(\n",
    "        sum_pop=('actor_popularity', 'sum')\n",
    "    ).reset_index()\n",
    "    grouped_datasets_by_genres.append(grouped_popularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d741592a7a37f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's see the correlation between the popularity of actors and the movie score for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1178072aa34a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:22:00.607310900Z",
     "start_time": "2023-11-17T13:22:00.537964300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('Correlation between sum of actor popularity and movie score for:\\n')\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for df in grouped_datasets_by_genres:\n",
    "    correlations.append({'genre': reduced_genre_dictionary[df['genre_id'].iloc[0]], 'corr': stats.spearmanr(df['movie_score'], df['sum_pop'])})\n",
    "    \n",
    "correlations.sort(key=lambda x: x['corr'][0], reverse=True)\n",
    "    \n",
    "for stat in correlations:\n",
    "    print(stat['genre'], ': ', stat['corr'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ddc1a1681b28d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 9) Movie directors' popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170af9634b50d88e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sentiment analysis of movies endings (IN MILESTONE 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
