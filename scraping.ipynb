{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.344895800Z",
     "start_time": "2023-11-16T11:24:51.032980200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.358904100Z",
     "start_time": "2023-11-16T11:24:51.344895800Z"
    }
   },
   "outputs": [],
   "source": [
    "#read a txt file and convert it to a dataframe\n",
    "def read_txt(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.834741200Z",
     "start_time": "2023-11-16T11:24:51.360903200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "plots = pd.read_csv('MovieSummaries/plot_summaries.txt',header=None, sep=\"\\t\")\n",
    "movies = pd.read_csv('MovieSummaries/movies_metadata.tsv',header=None, sep=\"\\t\")\n",
    "characters = pd.read_csv('MovieSummaries/characters_metadata.tsv',header=None, sep=\"\\t\")\n",
    "names = pd.read_csv('MovieSummaries/names_clusters.txt',header=None, sep=\"\\t\")\n",
    "tvtropes = pd.read_csv('MovieSummaries/tvtropes_clusters.txt',header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.850422Z",
     "start_time": "2023-11-16T11:24:52.835739700Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.columns = ['WikiID', 'Plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movies` data\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.885983400Z",
     "start_time": "2023-11-16T11:24:52.851421700Z"
    }
   },
   "outputs": [],
   "source": [
    "movies.columns = ['WikiID', 'FreebaseID', 'Name', 'ReleaseDate', 'Revenue', 'Runtime', 'Languages', 'Countries', 'Genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.959262600Z",
     "start_time": "2023-11-16T11:24:52.866130800Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting the years from the release date feature\n",
    "movie_with_date = movies[-movies[\"ReleaseDate\"].isna()].copy(deep=True)\n",
    "dates = movie_with_date[\"ReleaseDate\"]\n",
    "date_years = dates.astype(str).str.extract(r'(\\d{4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`characters` data:\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.987893700Z",
     "start_time": "2023-11-16T11:24:52.960258200Z"
    }
   },
   "outputs": [],
   "source": [
    "characters.columns = ['WikiID', 'FreebaseID', 'ReleaseDate', 'CharacterName', 'DateOfBirth', 'Gender', 'Height', 'Ethnicity', 'Name', 'AgeAtRealease', 'FreebaseCharacterActorMapID', 'FreebaseCharacterID', 'FreebaseActorID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Imbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:53.115180600Z",
     "start_time": "2023-11-16T11:24:52.974903100Z"
    }
   },
   "outputs": [],
   "source": [
    "import imdb_scraper as imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on how to use the scraper\n",
    "\n",
    "<b><span style=\"color:red\">Don't close the browser when it is in use </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.940577200Z",
     "start_time": "2023-11-16T11:24:53.110063900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'global_revenue': None,\n 'budget': 2846000.0,\n 'gross_domestic': None,\n 'opening_weekend': None,\n 'rating_score': 6.7,\n 'number_of_ratings': 1700.0,\n 'watched_rank': None,\n 'producer': 'Robert Wise',\n 'release_year': None}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the scraper object\n",
    "myscraper = imdb.ImdbScraper()\n",
    "\n",
    "# select a movie wikipedia id\n",
    "movie_id = movies.iloc[74623][\"WikiID\"]\n",
    "\n",
    "# scrape the movie infos\n",
    "scraped_data = myscraper.get_imdb_infos(movie_id)\n",
    "\n",
    "# close the browser\n",
    "myscraper.close()\n",
    "\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the scraping begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing `n_computer` partitions to parallelize ImDB's scraping on multiple computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.962899400Z",
     "start_time": "2023-11-15T15:24:34.924984800Z"
    }
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_computer  = 6\n",
    "size        = n//n_computer\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_computer computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_computer)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_computer*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.964898700Z",
     "start_time": "2023-11-15T15:24:34.939915200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 13623\n",
      "\n",
      "partition 0: (0, 13622)\n",
      "partition 1: (13623, 27245)\n",
      "partition 2: (27246, 40868)\n",
      "partition 3: (40869, 54491)\n",
      "partition 4: (54492, 68114)\n",
      "partition 5: (68115, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_computer)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each user should use its index to compute his attributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.979962Z",
     "start_time": "2023-11-15T15:24:34.953900400Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select your index here:\n",
    "    - Anthony   0 & 1\n",
    "    - Anton     2\n",
    "    - Aymeric   3\n",
    "    - Eric      4\n",
    "    - Yara      5\n",
    "\"\"\"\n",
    "index = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ImDB has a protection against bots, using `requests` yields a `forbidden` error. The use of `selenium` to simmulate a real human operator avoids this problem and allows to scrape, though more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def scrape_partition(index):\n",
    "    # create a header dataframe\n",
    "    header = pd.DataFrame(columns=[\"WikiID\", \"Name\", \"global_revenue\", \"budget\", \"gross_domestic\", \"opening_weekend\", \n",
    "                               \"rating_score\", \"number_of_ratings\", \"watched_rank\", \"producer\", \"release_year\"])  \n",
    "    \n",
    "    # csv file name\n",
    "    csv_file = 'MovieSummaries/imdb_scraped_data_' + str(index) + '.csv'\n",
    "    \n",
    "    #initialize the scraper object\n",
    "    myscraper = imdb.ImdbScraper()\n",
    "    \n",
    "    # if the csv_file doesn't exist, create it and write the header\n",
    "    if not os.path.isfile(csv_file):\n",
    "        header.to_csv(csv_file, index=False)\n",
    "        starting_index = 0\n",
    "    else:\n",
    "        scraped = pd.read_csv(csv_file)\n",
    "        starting_index = scraped.shape[0]\n",
    "       \n",
    "    index_range = partitions[index][starting_index:]\n",
    "    \n",
    "    for i in index_range:\n",
    "        movie_id = movies.iloc[i][\"WikiID\"]\n",
    "    \n",
    "        movie_infos = myscraper.get_imdb_infos(movie_id)\n",
    "        row = [movie_id, movies[\"Name\"].values[i], *list(movie_infos.values())]\n",
    "    \n",
    "        # create a new DataFrame for the row\n",
    "        row_df = pd.DataFrame([row], columns=header.columns)\n",
    "    \n",
    "        # replace None values with the string 'None'\n",
    "        row_df = row_df.fillna('None')\n",
    "    \n",
    "        # append the row DataFrame to the CSV file\n",
    "        row_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    myscraper.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.960543100Z",
     "start_time": "2023-11-16T11:24:58.945546300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "scrape_partition(index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T03:23:24.344054Z",
     "start_time": "2023-11-15T15:24:36.735759900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallelized version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_threads  = 8\n",
    "size        = n//n_threads\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_threads computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_threads)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_threads*size:] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:01.596453500Z",
     "start_time": "2023-11-16T11:25:01.569032400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 4087\n",
      "\n",
      "partition 0: (0, 4086)\n",
      "partition 1: (4087, 8173)\n",
      "partition 2: (8174, 12260)\n",
      "partition 3: (12261, 16347)\n",
      "partition 4: (16348, 20434)\n",
      "partition 5: (20435, 24521)\n",
      "partition 6: (24522, 28608)\n",
      "partition 7: (28609, 32695)\n",
      "partition 8: (32696, 36782)\n",
      "partition 9: (36783, 40869)\n",
      "partition 10: (40870, 44956)\n",
      "partition 11: (44957, 49043)\n",
      "partition 12: (49044, 53130)\n",
      "partition 13: (53131, 57217)\n",
      "partition 14: (57218, 61304)\n",
      "partition 15: (61305, 65391)\n",
      "partition 16: (65392, 69478)\n",
      "partition 17: (69479, 73565)\n",
      "partition 18: (73566, 77652)\n",
      "partition 19: (77653, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_threads)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:03.919272900Z",
     "start_time": "2023-11-16T11:25:03.912956400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "threads = []\n",
    "\n",
    "for i in range(n_threads):\n",
    "    threads.append(threading.Thread(target=scrape_partition, args=(i,)))\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-16T11:25:08.152292Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the scraped partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(1, n_computer):\\n    partition = partition_file + str(i) + '.csv'\\n    part_df = pd.concat([part_df, pd.read_csv(partition)], axis=0)\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = r\"./MovieSummaries/imdb_scraped_dataset.csv\"\n",
    "try:\n",
    "    partition_dataset = pd.read_csv(csv_file)\n",
    "except:\n",
    "    partition_file = r'partitions/imdb_scraped_data_'\n",
    "    partition_dataset = pd.read_csv(partition_file + '0.csv')\n",
    "\n",
    "    for i in range(1, n_computer):\n",
    "        partition = partition_file + str(i) + '.csv'\n",
    "        partition_dataset = pd.concat([partition_dataset, pd.read_csv(partition)], axis=0)\n",
    "        partition_dataset.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMBD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmdb_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tmdb_id': 11,\n",
       " 'movie_name': 'Star Wars : Episode IV - A New Hope',\n",
       " 'release_date': '1977-05-25',\n",
       " 'revenue': 775398007,\n",
       " 'budget': 11000000,\n",
       " 'rating': 8.204,\n",
       " 'vote_count': 19278,\n",
       " 'popularity': 86.431,\n",
       " 'runtime': 121,\n",
       " 'production': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "my_tmdb_scraper = tmdb_scraper.tmdb_scraper()\n",
    "my_tmdb_scraper.get_tmdb_infos(\"Star Wars : Episode IV - A New Hope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names   = movies[\"Name\"].values\n",
    "\n",
    "# create an empty DataFrame for the first row\n",
    "header = pd.DataFrame(columns=[\"tmdb_id\", \"movie_name\", \"release_date\", \"revenue\", \"budget\", \"rating\", \"vote_count\", \"popularity\", \"runtime\", \"production\"])\n",
    "\n",
    "csv_file = 'MovieSummaries/tmbd_scraped_data.csv'\n",
    "\n",
    "# if the csv_file doesn't exist yet, make a header\n",
    "if not os.path.isfile(csv_file):\n",
    "    # write the DataFrame to a CSV file\n",
    "    header.to_csv(csv_file, index=False)\n",
    "\n",
    "index_range = np.arange(0, 10)\n",
    "\n",
    "for i in index_range:\n",
    "    name = dataset_names[i]\n",
    "\n",
    "    movie_infos = my_tmdb_scraper.get_tmdb_infos(name)\n",
    "    row = [*list(movie_infos.values())]\n",
    "\n",
    "    # create a new DataFrame for the row\n",
    "    row_df = pd.DataFrame([row], columns=header.columns)\n",
    "\n",
    "    # replace None values with the string 'None'\n",
    "    row_df = row_df.fillna('None')\n",
    "\n",
    "    # append the row DataFrame to the CSV file\n",
    "    row_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "    #print(f\"{name}  : {movie_infos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"https://en.wikipedia.org/w/api.php?action=query&pageids=975900&format=json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': '',\n",
       " 'query': {'pages': {'975900': {'pageid': 975900,\n",
       "    'ns': 0,\n",
       "    'title': 'Ghosts of Mars'}}}}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tmdbsimple as tmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name = \"My Friend Ganesha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\antho\\Desktop\\EPFL\\ADA_Milestone_2\\test.ipynb Cell 21\u001B[0m line \u001B[0;36m1\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ADA_Milestone_2/test.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m popularity  \u001B[39m=\u001B[39m result[\u001B[39m'\u001B[39m\u001B[39mpopularity\u001B[39m\u001B[39m'\u001B[39m]\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ADA_Milestone_2/test.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001B[0m runtime     \u001B[39m=\u001B[39m result[\u001B[39m'\u001B[39m\u001B[39mruntime\u001B[39m\u001B[39m'\u001B[39m]\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ADA_Milestone_2/test.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m production  \u001B[39m=\u001B[39m result[\u001B[39m'\u001B[39;49m\u001B[39mproduction_companies\u001B[39;49m\u001B[39m'\u001B[39;49m][\u001B[39m0\u001B[39;49m][\u001B[39m'\u001B[39m\u001B[39mname\u001B[39m\u001B[39m'\u001B[39m]\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ADA_Milestone_2/test.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m release_date \u001B[39m=\u001B[39m result[\u001B[39m'\u001B[39m\u001B[39mrelease_date\u001B[39m\u001B[39m'\u001B[39m]\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tmdb.REQUESTS_SESSION = requests.Session()\n",
    "search      = tmdb.Search()\n",
    "\n",
    "search.movie(query=movie_name)\n",
    "tmdb_id     = search.results[0]['id']\n",
    "result      = tmdb.Movies(tmdb_id).info()\n",
    "\n",
    "revenue     = result['revenue']\n",
    "budget      = result['budget']\n",
    "rating      = result['vote_average']\n",
    "vote_count  = result['vote_count']\n",
    "popularity  = result['popularity']\n",
    "runtime     = result['runtime']\n",
    "production  = result['production_companies'][0]['name']\n",
    "release_date = result['release_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ghosts of Mars 2001-08-24'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name = dataset_names[0]\n",
    "movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# access the webpage\n",
    "driver.get(\"https://www.imdb.com/\")\n",
    "\n",
    "# find the search bar & search button\n",
    "search_bar = driver.find_element(\"xpath\", '//*[@id=\"suggestion-search\"]')\n",
    "search_button = driver.find_element(\"xpath\", '//*[@id=\"suggestion-search-button\"]')\n",
    "\n",
    "# search for the movie\n",
    "search_bar.send_keys(movie_name)\n",
    "search_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = soup.find_all(\"span\", {\"class\": \"ipc-metadata-list-summary-item__li\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">2001</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Natasha Henstridge, Ice Cube</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">2001</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Vidéo</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">John Carpenter</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">2001</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Vidéo</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Anthrax, Tiago Becker</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">2001</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Vidéo</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Danielle Burgio, John Carpenter</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">2020</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">S1.E23</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Épisode de podcast</span>,\n",
       " <span aria-disabled=\"false\" class=\"ipc-metadata-list-summary-item__li\">Série de podcast</span>]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WikiID                                                    175026\n",
       "FreebaseID                                             /m/017n1p\n",
       "Name                                               Sarah and Son\n",
       "ReleaseDate                                                 1930\n",
       "Revenue                                                      NaN\n",
       "Runtime                                                     86.0\n",
       "Languages                     {\"/m/02h40lc\": \"English Language\"}\n",
       "Countries              {\"/m/09c7w0\": \"United States of America\"}\n",
       "Genres         {\"/m/07s9rl0\": \"Drama\", \"/m/01g6gs\": \"Black-an...\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the first result\n",
    "first_result = driver.find_element(\"xpath\", '//*[@id=\"__next\"]/main/div[2]/div[3]/section/div/div[1]/section[2]/div[2]/ul/li[1]')\n",
    "first_result.click()\n",
    "\n",
    "#parse the resulting webpage\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "box_office = soup.find('div', {'data-testid': \"title-boxoffice-section\"})\n",
    "try:\n",
    "    brut_li                 = box_office.find('li', {'data-testid' : \"title-boxoffice-cumulativeworldwidegross\"})\n",
    "    global_revenue_raw      = brut_li.find('span', {'class' : \"ipc-metadata-list-item__list-content-item\"}).text\n",
    "    global_revenue          = int(re.findall(r'\\d+', global_revenue_raw.replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    global_revenue          = None\n",
    "\n",
    "try:\n",
    "    budget_li               = box_office.find('li', {'data-testid' : \"title-boxoffice-budget\"})\n",
    "    budget_raw              = budget_li.find('span', {'class' : \"ipc-metadata-list-item__list-content-item\"}).text\n",
    "    budget                  = int(re.findall(r'\\d+', budget_raw.replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    budget = None\n",
    "\n",
    "try:\n",
    "    gross_domestic_li       = box_office.find('li', {'data-testid' : \"title-boxoffice-grossdomestic\"})\n",
    "    gross_domestic_raw      = gross_domestic_li.find('span', {'class' : \"ipc-metadata-list-item__list-content-item\"}).text\n",
    "    gross_domestic          = int(re.findall(r'\\d+', gross_domestic_raw.replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    gross_domestic          = None\n",
    "\n",
    "try:\n",
    "    opening_weekend_li      = box_office.find('li', {'data-testid' : \"title-boxoffice-openingweekenddomestic\"})\n",
    "    opening_weekend_raw     = opening_weekend_li.find('span', {'class' : \"ipc-metadata-list-item__list-content-item\"}).text\n",
    "    opening_weekend         = int(re.findall(r'\\d+', opening_weekend_raw.replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    opening_weekend         = None\n",
    "\n",
    "try:\n",
    "    rating_score_div        = soup.find('div', {'data-testid' : \"hero-rating-bar__aggregate-rating__score\"})\n",
    "    rating_score_raw        = rating_score_div.text\n",
    "    rating_score            = float(re.findall(r'\\d{1}.\\d{1}', rating_score_raw.replace(',', '.').replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    rating_score            = None\n",
    "\n",
    "try:\n",
    "    number_of_ratings_div   = soup.find('div', {'data-testid' : \"hero-rating-bar__aggregate-rating\"})\n",
    "    number_of_ratings_raw   = number_of_ratings_div.find('div', {'class' : \"sc-bde20123-3 bjjENQ\"}).text\n",
    "    number_of_ratings       = float(re.findall(r'\\d+.?\\d?', number_of_ratings_raw.replace(',', '.').replace('\\u202f', ''))[0])\n",
    "\n",
    "    rating_unit             = number_of_ratings_raw[-1]\n",
    "    if (rating_unit=='M'):\n",
    "        number_of_ratings   = 1000000*number_of_ratings\n",
    "    elif (rating_unit=='k'):\n",
    "        number_of_ratings   = 1000*number_of_ratings\n",
    "except:\n",
    "    number_of_ratings       = None\n",
    "\n",
    "try:\n",
    "    watched_rank_div        = soup.find('div', {'data-testid' : \"hero-rating-bar__popularity\"})\n",
    "    watched_rank_raw        = watched_rank_div.find('div', {'data-testid' : \"hero-rating-bar__popularity__score\"}).text\n",
    "    watched_rank            = int(re.findall(r'\\d+', watched_rank_raw.replace('\\u202f', ''))[0])\n",
    "except:\n",
    "    watched_rank            = None\n",
    "\n",
    "try:\n",
    "    cast_section            = soup.find('section', {\"data-testid\" : \"title-cast\"})\n",
    "    cast_ul                 = cast_section.find('ul', {\"class\" : \"ipc-metadata-list ipc-metadata-list--dividers-all sc-bfec09a1-8 iiDmgX ipc-metadata-list--base\"})\n",
    "    cast_raw                = cast_ul.find('a', {'class' : 'ipc-metadata-list-item__list-content-item ipc-metadata-list-item__list-content-item--link'}).text\n",
    "    producer               = cast_raw\n",
    "except:\n",
    "    producer                = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = movies.iloc[5][\"WikiID\"]\n",
    "resp = requests.get(f\"http://en.wikipedia.org/w/api.php?action=query&prop=info&pageids={movie_id}&inprop=url&format=json\")\n",
    "wikipedia_movie_link = resp.json()['query']['pages'][str(movie_id)]['fullurl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': '',\n",
       " 'query': {'pages': {'13696889': {'pageid': 13696889,\n",
       "    'ns': 0,\n",
       "    'title': 'The Gangsters',\n",
       "    'contentmodel': 'wikitext',\n",
       "    'pagelanguage': 'en',\n",
       "    'pagelanguagehtmlcode': 'en',\n",
       "    'pagelanguagedir': 'ltr',\n",
       "    'touched': '2023-11-07T16:37:20Z',\n",
       "    'lastrevid': 1112156191,\n",
       "    'length': 1639,\n",
       "    'fullurl': 'https://en.wikipedia.org/wiki/The_Gangsters',\n",
       "    'editurl': 'https://en.wikipedia.org/w/index.php?title=The_Gangsters&action=edit',\n",
       "    'canonicalurl': 'https://en.wikipedia.org/wiki/The_Gangsters'}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/The_Gangsters'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikipedia_movie_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(wikipedia_movie_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse wikipedia_movie_link\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "tools = soup.find_all('div', {'id': 'vector-page-tools'})\n",
    "wikidata_link = tools[0].find('li', {'id':'t-wikibase'}).find('a')['href']\n",
    "\n",
    "wikidata_html = requests.get(wikidata_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.imdb.com/title/tt0002894/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_soup = BeautifulSoup(wikidata_html.text, 'html.parser')\n",
    "wiki_imdb = new_soup.find('div', {'id': 'P345'})\n",
    "imdb_id = wiki_imdb.find('div', {'class': 'wikibase-snakview-value wikibase-snakview-variation-valuesnak'}).text\n",
    "imdb_link = \"https://www.imdb.com/title/\" + str(imdb_id) + \"/\"\n",
    "imdb_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://wikidata-externalid-url.toolforge.org/?p=345&url_prefix=https://www.imdb.com/&id=tt0002894'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb_scraper as imdb\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how to use the scraper\n",
    "myscraper = imdb.ImdbScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "movie_id = movies.iloc[24975][\"WikiID\"]\n",
    "myscraper.get_imdb_infos(movie_id)\n",
    "\n",
    "imdb_html = myscraper.driver.page_source\n",
    "soup = BeautifulSoup(imdb_html, 'html.parser')\n",
    "\n",
    "soup.find('div', {'class': 'sc-dffc6c81-0 grcyBP'}).find('a', {'class': 'ipc-link ipc-link--baseAlt ipc-link--inherit-color'}).text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
