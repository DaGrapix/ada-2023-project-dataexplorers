{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.344895800Z",
     "start_time": "2023-11-16T11:24:51.032980200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/MovieSummaries/'\n",
    "ADDITIONAL_FOLDER = 'data/AdditionalData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.358904100Z",
     "start_time": "2023-11-16T11:24:51.344895800Z"
    }
   },
   "outputs": [],
   "source": [
    "#read a txt file and convert it to a dataframe\n",
    "def read_txt(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.834741200Z",
     "start_time": "2023-11-16T11:24:51.360903200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "plots = pd.read_csv(DATA_FOLDER + 'plot_summaries.txt',header=None, sep=\"\\t\")\n",
    "movies = pd.read_csv(DATA_FOLDER + 'movie.metadata.tsv',header=None, sep=\"\\t\")\n",
    "characters = pd.read_csv(DATA_FOLDER + 'character.metadata.tsv',header=None, sep=\"\\t\")\n",
    "names = pd.read_csv(DATA_FOLDER + 'name.clusters.txt',header=None, sep=\"\\t\")\n",
    "tvtropes = pd.read_csv(DATA_FOLDER + 'tvtropes.clusters.txt',header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.850422Z",
     "start_time": "2023-11-16T11:24:52.835739700Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.columns = ['wikipedia_movie_id', 'plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movies` data\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.885983400Z",
     "start_time": "2023-11-16T11:24:52.851421700Z"
    }
   },
   "outputs": [],
   "source": [
    "movies.columns = ['wikipedia_movie_id', 'freebase_movie_id', 'name', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`characters` data:\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.987893700Z",
     "start_time": "2023-11-16T11:24:52.960258200Z"
    }
   },
   "outputs": [],
   "source": [
    "characters.columns = ['wikipedia_movie_id', 'freebase_movie_id', 'release_date', 'character_name', 'date_of_birth', 'gender', 'height', 'ethnicity', 'name', 'age_at_release', 'freebase_character_map_id', 'freebase_caracter_id', 'freebase_actor_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:53.115180600Z",
     "start_time": "2023-11-16T11:24:52.974903100Z"
    }
   },
   "outputs": [],
   "source": [
    "import imdb_scraper as imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on how to use the scraper\n",
    "\n",
    "<b><span style=\"color:red\">Don't close the browser when it is in use </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.940577200Z",
     "start_time": "2023-11-16T11:24:53.110063900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global_revenue': None,\n",
       " 'budget': 2846000.0,\n",
       " 'gross_domestic': None,\n",
       " 'opening_weekend': None,\n",
       " 'rating_score': 6.7,\n",
       " 'number_of_ratings': 1700.0,\n",
       " 'watched_rank': None,\n",
       " 'producer': 'Robert Wise',\n",
       " 'release_year': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the scraper object\n",
    "myscraper = imdb.ImdbScraper()\n",
    "\n",
    "# select a movie wikipedia id\n",
    "movie_id = movies.iloc[74623][\"wikipedia_movie_id\"]\n",
    "\n",
    "# scrape the movie infos\n",
    "scraped_data = myscraper.get_imdb_infos(movie_id)\n",
    "\n",
    "# close the browser\n",
    "myscraper.close()\n",
    "\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the scraping begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing `n_computer` partitions to parallelize ImDB's scraping on multiple computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.962899400Z",
     "start_time": "2023-11-15T15:24:34.924984800Z"
    }
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_computer  = 6\n",
    "size        = n//n_computer\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_computer computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_computer)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_computer*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.964898700Z",
     "start_time": "2023-11-15T15:24:34.939915200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 13623\n",
      "\n",
      "partition 0: (0, 13622)\n",
      "partition 1: (13623, 27245)\n",
      "partition 2: (27246, 40868)\n",
      "partition 3: (40869, 54491)\n",
      "partition 4: (54492, 68114)\n",
      "partition 5: (68115, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_computer)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each user should use its index to compute his attributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.979962Z",
     "start_time": "2023-11-15T15:24:34.953900400Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select your index here:\n",
    "    - Anthony   0 & 1\n",
    "    - Anton     2\n",
    "    - Aymeric   3\n",
    "    - Eric      4\n",
    "    - Yara      5\n",
    "\"\"\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ImDB has a protection against bots, using `requests` yields a `forbidden` error. The use of `selenium` to simmulate a real human operator avoids this problem and allows to scrape, though more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.960543100Z",
     "start_time": "2023-11-16T11:24:58.945546300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_partition(index):\n",
    "    \"\"\"\n",
    "    Scrape the IMDB data for the movies in the partition with the given index.\n",
    "    The scraped dataset is saved in a separate csv file for each partition.\n",
    "\n",
    "    Input:\n",
    "    index(int) : The index of the partition to scrape.\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(ADDITIONAL_FOLDER + 'imdb_partitions')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # create a header dataframe\n",
    "    header = pd.DataFrame(columns=[\"wikipedia_movie_id\", \"name\", \"global_revenue\", \"budget\", \"gross_domestic\", \"opening_weekend\", \n",
    "                               \"rating_score\", \"number_of_ratings\", \"watched_rank\", \"producer\", \"release_year\"])  \n",
    "    \n",
    "    # csv file name\n",
    "    csv_file = ADDITIONAL_FOLDER + 'imdb_partitions/imdb_scraped_data_' + str(index) + '.csv'\n",
    "    \n",
    "    #initialize the scraper object\n",
    "    myscraper = imdb.ImdbScraper()\n",
    "    \n",
    "    # if the csv_file doesn't exist, create it and write the header\n",
    "    if not os.path.isfile(csv_file):\n",
    "        header.to_csv(csv_file, index=False)\n",
    "        starting_index = 0\n",
    "    else:\n",
    "        scraped = pd.read_csv(csv_file)\n",
    "        starting_index = scraped.shape[0]\n",
    "       \n",
    "    index_range = partitions[index][starting_index:]\n",
    "    \n",
    "    for i in index_range:\n",
    "        movie_id = movies.iloc[i][\"wikipedia_movie_id\"]\n",
    "    \n",
    "        movie_infos = myscraper.get_imdb_infos(movie_id)\n",
    "        row = [movie_id, movies[\"name\"].values[i], *list(movie_infos.values())]\n",
    "    \n",
    "        # create a new DataFrame for the row\n",
    "        row_df = pd.DataFrame([row], columns=header.columns)\n",
    "    \n",
    "        # replace None values with the string 'None'\n",
    "        row_df = row_df.fillna('None')\n",
    "    \n",
    "        # append the row DataFrame to the CSV file\n",
    "        row_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    myscraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T03:23:24.344054Z",
     "start_time": "2023-11-15T15:24:36.735759900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run the scraper\n",
    "scrape_partition(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multi-processed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows an implementation of the scraper running in parallel on multiple threads on a single computer. The number of threads `n_threads` can be increased if the user's computer has access to more threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:01.596453500Z",
     "start_time": "2023-11-16T11:25:01.569032400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_threads   = 6\n",
    "size        = n//n_threads\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_threads computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_threads)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_threads*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:03.919272900Z",
     "start_time": "2023-11-16T11:25:03.912956400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 81741\n",
      "partitions' size: 13623\n",
      "\n",
      "partition 0: (0, 13622)\n",
      "partition 1: (13623, 27245)\n",
      "partition 2: (27246, 40868)\n",
      "partition 3: (40869, 54491)\n",
      "partition 4: (54492, 68114)\n",
      "partition 5: (68115, 81740)\n"
     ]
    }
   ],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_threads)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-16T11:25:08.152292Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "threads = []\n",
    "\n",
    "for i in range(n_threads):\n",
    "    threads.append(threading.Thread(target=scrape_partition, args=(i,)))\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the scraped partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = ADDITIONAL_FOLDER + r\"/imdb_scraped_dataset.csv\"\n",
    "n_computer  = 6\n",
    "try:\n",
    "    partition_dataset = pd.read_csv(csv_file)\n",
    "except:\n",
    "    partition_file = ADDITIONAL_FOLDER + r'imdb_partitions/imdb_scraped_data_'\n",
    "    partition_dataset = pd.read_csv(partition_file + '0.csv')\n",
    "\n",
    "    for i in range(1, n_computer):\n",
    "        partition = partition_file + str(i) + '.csv'\n",
    "        partition_dataset = pd.concat([partition_dataset, pd.read_csv(partition)], axis=0).fillna('None')\n",
    "        partition_dataset.to_csv(csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
