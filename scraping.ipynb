{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.344895800Z",
     "start_time": "2023-11-16T11:24:51.032980200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:51.358904100Z",
     "start_time": "2023-11-16T11:24:51.344895800Z"
    }
   },
   "outputs": [],
   "source": [
    "#read a txt file and convert it to a dataframe\n",
    "def read_txt(path):\n",
    "    df = pd.read_csv(path, sep='\\t', header=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.834741200Z",
     "start_time": "2023-11-16T11:24:51.360903200Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "plots = pd.read_csv('MovieSummaries/plot_summaries.txt',header=None, sep=\"\\t\")\n",
    "movies = pd.read_csv('MovieSummaries/movies_metadata.tsv',header=None, sep=\"\\t\")\n",
    "characters = pd.read_csv('MovieSummaries/characters_metadata.tsv',header=None, sep=\"\\t\")\n",
    "names = pd.read_csv('MovieSummaries/names_clusters.txt',header=None, sep=\"\\t\")\n",
    "tvtropes = pd.read_csv('MovieSummaries/tvtropes_clusters.txt',header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.850422Z",
     "start_time": "2023-11-16T11:24:52.835739700Z"
    }
   },
   "outputs": [],
   "source": [
    "plots.columns = ['WikiID', 'Plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movies` data\n",
    "\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie name\n",
    "4. Movie release date\n",
    "5. Movie box office revenue\n",
    "6. Movie runtime\n",
    "7. Movie languages (Freebase ID:name tuples)\n",
    "8. Movie countries (Freebase ID:name tuples)\n",
    "9. Movie genres (Freebase ID:name tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.885983400Z",
     "start_time": "2023-11-16T11:24:52.851421700Z"
    }
   },
   "outputs": [],
   "source": [
    "movies.columns = ['WikiID', 'FreebaseID', 'Name', 'ReleaseDate', 'Revenue', 'Runtime', 'Languages', 'Countries', 'Genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.959262600Z",
     "start_time": "2023-11-16T11:24:52.866130800Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting the years from the release date feature\n",
    "movie_with_date = movies[-movies[\"ReleaseDate\"].isna()].copy(deep=True)\n",
    "dates = movie_with_date[\"ReleaseDate\"]\n",
    "date_years = dates.astype(str).str.extract(r'(\\d{4})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`characters` data:\n",
    "1. Wikipedia movie ID\n",
    "2. Freebase movie ID\n",
    "3. Movie release date\n",
    "4. Character name\n",
    "5. Actor date of birth\n",
    "6. Actor gender\n",
    "7. Actor height (in meters)\n",
    "8. Actor ethnicity (Freebase ID)\n",
    "9. Actor name\n",
    "10. Actor age at movie release\n",
    "11. Freebase character/actor map ID\n",
    "12. Freebase character ID\n",
    "13. Freebase actor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:52.987893700Z",
     "start_time": "2023-11-16T11:24:52.960258200Z"
    }
   },
   "outputs": [],
   "source": [
    "characters.columns = ['WikiID', 'FreebaseID', 'ReleaseDate', 'CharacterName', 'DateOfBirth', 'Gender', 'Height', 'Ethnicity', 'Name', 'AgeAtRealease', 'FreebaseCharacterActorMapID', 'FreebaseCharacterID', 'FreebaseActorID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:53.115180600Z",
     "start_time": "2023-11-16T11:24:52.974903100Z"
    }
   },
   "outputs": [],
   "source": [
    "import imdb_scraper as imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on how to use the scraper\n",
    "\n",
    "<b><span style=\"color:red\">Don't close the browser when it is in use </span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.940577200Z",
     "start_time": "2023-11-16T11:24:53.110063900Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the scraper object\n",
    "myscraper = imdb.ImdbScraper()\n",
    "\n",
    "# select a movie wikipedia id\n",
    "movie_id = movies.iloc[74623][\"WikiID\"]\n",
    "\n",
    "# scrape the movie infos\n",
    "scraped_data = myscraper.get_imdb_infos(movie_id)\n",
    "\n",
    "# close the browser\n",
    "myscraper.close()\n",
    "\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the scraping begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing `n_computer` partitions to parallelize ImDB's scraping on multiple computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.962899400Z",
     "start_time": "2023-11-15T15:24:34.924984800Z"
    }
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_computer  = 6\n",
    "size        = n//n_computer\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_computer computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_computer)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_computer*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.964898700Z",
     "start_time": "2023-11-15T15:24:34.939915200Z"
    }
   },
   "outputs": [],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_computer)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each user should use its index to compute his attributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:24:34.979962Z",
     "start_time": "2023-11-15T15:24:34.953900400Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select your index here:\n",
    "    - Anthony   0 & 1\n",
    "    - Anton     2\n",
    "    - Aymeric   3\n",
    "    - Eric      4\n",
    "    - Yara      5\n",
    "\"\"\"\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ImDB has a protection against bots, using `requests` yields a `forbidden` error. The use of `selenium` to simmulate a real human operator avoids this problem and allows to scrape, though more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:24:58.960543100Z",
     "start_time": "2023-11-16T11:24:58.945546300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_partition(index):\n",
    "    # create a header dataframe\n",
    "    header = pd.DataFrame(columns=[\"WikiID\", \"Name\", \"global_revenue\", \"budget\", \"gross_domestic\", \"opening_weekend\", \n",
    "                               \"rating_score\", \"number_of_ratings\", \"watched_rank\", \"producer\", \"release_year\"])  \n",
    "    \n",
    "    # csv file name\n",
    "    csv_file = 'MovieSummaries/imdb_scraped_data_' + str(index) + '.csv'\n",
    "    \n",
    "    #initialize the scraper object\n",
    "    myscraper = imdb.ImdbScraper()\n",
    "    \n",
    "    # if the csv_file doesn't exist, create it and write the header\n",
    "    if not os.path.isfile(csv_file):\n",
    "        header.to_csv(csv_file, index=False)\n",
    "        starting_index = 0\n",
    "    else:\n",
    "        scraped = pd.read_csv(csv_file)\n",
    "        starting_index = scraped.shape[0]\n",
    "       \n",
    "    index_range = partitions[index][starting_index:]\n",
    "    \n",
    "    for i in index_range:\n",
    "        movie_id = movies.iloc[i][\"WikiID\"]\n",
    "    \n",
    "        movie_infos = myscraper.get_imdb_infos(movie_id)\n",
    "        row = [movie_id, movies[\"Name\"].values[i], *list(movie_infos.values())]\n",
    "    \n",
    "        # create a new DataFrame for the row\n",
    "        row_df = pd.DataFrame([row], columns=header.columns)\n",
    "    \n",
    "        # replace None values with the string 'None'\n",
    "        row_df = row_df.fillna('None')\n",
    "    \n",
    "        # append the row DataFrame to the CSV file\n",
    "        row_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    myscraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T03:23:24.344054Z",
     "start_time": "2023-11-15T15:24:36.735759900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scrape_partition(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parallelized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part shows an implementation of the scraper on multiple threads on a single computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:01.596453500Z",
     "start_time": "2023-11-16T11:25:01.569032400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n           = movies.shape[0]\n",
    "n_threads   = 8\n",
    "size        = n//n_threads\n",
    "\n",
    "# create a uniform partition of indices from 0 to n-1 for n_threads computers\n",
    "indices         = [i for i in range(n)]\n",
    "partitions      = [indices[i*size:(i+1)*size] for i in range(n_threads)]\n",
    "partitions[-1]  = partitions[-1] + indices[n_threads*size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T11:25:03.919272900Z",
     "start_time": "2023-11-16T11:25:03.912956400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show first and last element of each partition\n",
    "partition_intervals = [(partitions[i][0], partitions[i][-1]) for i in range(n_threads)]\n",
    "print(f\"number of elements: {n}\")\n",
    "print(f\"partitions' size: {size}\\n\")\n",
    "for i in range(len(partition_intervals)):\n",
    "    print(f\"partition {i}: {partition_intervals[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-16T11:25:08.152292Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "threads = []\n",
    "\n",
    "for i in range(n_threads):\n",
    "    threads.append(threading.Thread(target=scrape_partition, args=(i,)))\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the scraped partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = r\"./MovieSummaries/imdb_scraped_dataset.csv\"\n",
    "n_computer  = 6\n",
    "try:\n",
    "    partition_dataset = pd.read_csv(csv_file)\n",
    "except:\n",
    "    partition_file = r'partitions/imdb_scraped_data_'\n",
    "    partition_dataset = pd.read_csv(partition_file + '0.csv')\n",
    "\n",
    "    for i in range(1, n_computer):\n",
    "        partition = partition_file + str(i) + '.csv'\n",
    "        partition_dataset = pd.concat([partition_dataset, pd.read_csv(partition)], axis=0).fillna('None')\n",
    "        partition_dataset.to_csv(csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
